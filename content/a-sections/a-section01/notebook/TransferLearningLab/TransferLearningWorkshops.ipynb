{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wKuC9YnAQTfB"
   },
   "source": [
    "\n",
    "<h2 align=\"left\"> \n",
    "<img src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\" width=\"50\">\n",
    "</h2>   \n",
    "\n",
    "\n",
    "## [**IACS: ComputeFest 2020**](https://www.computefest.seas.harvard.edu/)\n",
    "\n",
    "#### **Authors/Instructors:**\n",
    "  Pavlos Protopapas,   Marios Mattheakis, Robbert Struyven,  Camilo Fosco, Vincent Casser\n",
    "\n",
    "\n",
    "\n",
    "------------\n",
    "\n",
    "\n",
    "# **Workshop 1 Transfer Learning** \n",
    "\n",
    "## How to use existing models for transfer learning \n",
    "\n",
    "------------\n",
    "\n",
    "Welcome to day 2 of ComputeFest 2020! In this workshop, we will explore and play around with the common applications of transfer learning. Transfer learning can be used to improve performance on small datasets, across tasks, and even to recognize and properly process unseen examples.\n",
    "The flow of the workshop is as follows:\n",
    "\n",
    "1. **The basics of transfer learning**\n",
    "\n",
    "2. Transfer learning from classification to segmentation\n",
    "\n",
    "3. Transfer learning through distillation\n",
    "\n",
    "\n",
    "## **Goal of the Lab 1**\n",
    "\n",
    "- Learn the idea of transfer learning through a simple classification example\n",
    "- Build  new networks by using pre-trained models. Combine  pre-trained layers with new untrained layers and train specific layers.\n",
    "- Get good performance in image classification and  feature extraction with training in small datasets.  \n",
    "\n",
    "## **Structure**\n",
    "\n",
    "0.  Setup\n",
    "\n",
    "1. Problem Statement\n",
    "  \n",
    "  1.1 Performance of the pre-trained  *mobileNet*  \n",
    "      - Different performance for different categories  \n",
    "\n",
    "  1.2 Train the mobileNet from scratch (`mobile_from_scratch`)\n",
    "\n",
    "2. Transfer Learning to the rescue\n",
    "\n",
    "  2.1 Inspect the extracted features from the pre-trained mobileNet\n",
    "\n",
    "  2.2 Add and train a new dense layer on top of the pre-trained mobileNet (`mobile_1_layer`)\n",
    "\n",
    "  2.3 Re-train the last-2 layers of the pre-trained mobileNet (`mobile_2_layers`)\n",
    "\n",
    "  2.4 Re-train all layers of pre-trained mobileNet \n",
    "(`mobile_all_layers`)\n",
    "\n",
    "  2.5 Fine-tuning of the pre-trained mobileNet (`mobile_tuned`)\n",
    "\n",
    "----------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KFuWR8KKoYbF"
   },
   "source": [
    "# **0. Setup**\n",
    "\n",
    "Setup the workspace. Connect the colab with the google drive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 35478,
     "status": "ok",
     "timestamp": 1579709340096,
     "user": {
      "displayName": "Alexandra Dumitriu",
      "photoUrl": "",
      "userId": "05284431433817975591"
     },
     "user_tz": 300
    },
    "id": "qyWizQJ7nhw1",
    "outputId": "001d97bc-1be7-4673-87b7-029a09eca3c9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WSiXtiePuRxJ"
   },
   "source": [
    "**Please make sure that you have loaded the data in your drive**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6GY6FPGytyhw"
   },
   "source": [
    "![alt text](https://i.imgur.com/bxdBm9r.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Tm5GdbjD572"
   },
   "source": [
    "Import the packages and confirm that you import version 2 of tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9199,
     "status": "ok",
     "timestamp": 1579709452742,
     "user": {
      "displayName": "Alexandra Dumitriu",
      "photoUrl": "",
      "userId": "05284431433817975591"
     },
     "user_tz": 300
    },
    "id": "k9cTztTfsg0O",
    "outputId": "92afa69f-446f-4cb1-f5fb-443bb7e0448b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorflow_version` not found.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Import the TF V2 through this magic command\n",
    "%tensorflow_version 2.x\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import backend as K\n",
    "from PIL import Image\n",
    "\n",
    "# Check the Version of the Tensor Flow. It should be the version 2\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zjoNzdvjEJ2J"
   },
   "source": [
    "If we have version 2 of TF we proceed by importing Keras and necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 450,
     "status": "ok",
     "timestamp": 1579709460635,
     "user": {
      "displayName": "Alexandra Dumitriu",
      "photoUrl": "",
      "userId": "05284431433817975591"
     },
     "user_tz": 300
    },
    "id": "-w0A4FMO0sl0",
    "outputId": "fc01677c-9a0c-44cd-d066-00214f1345c9"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "from tensorflow.keras.layers import Dense,GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "from IPython.display import Image\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z-DNLzEgixpc"
   },
   "source": [
    "Colab allows us to use shell commands in the drive directories, such as \"ls\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2069,
     "status": "ok",
     "timestamp": 1579709467718,
     "user": {
      "displayName": "Alexandra Dumitriu",
      "photoUrl": "",
      "userId": "05284431433817975591"
     },
     "user_tz": 300
    },
    "id": "4OmMs0ULZsS_",
    "outputId": "b27dd1ae-d4ee-434b-d2fd-568369fd6d7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access 'drive/My Drive/TransferLearningLab/': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "ls  drive/My\\ Drive/TransferLearningLab/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j6nTCfFdPodf"
   },
   "source": [
    "Set the main path for the **working directory**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g2yIp5S4PtgJ"
   },
   "outputs": [],
   "source": [
    "pathFolder = 'data_Lab1/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1k5j9daWiqNO"
   },
   "source": [
    "Explore your dataset (images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1760,
     "status": "ok",
     "timestamp": 1579709481888,
     "user": {
      "displayName": "Alexandra Dumitriu",
      "photoUrl": "",
      "userId": "05284431433817975591"
     },
     "user_tz": 300
    },
    "id": "Osd1qW4c9ZQR",
    "outputId": "9c409642-4f46-45bc-c88b-cf35a2406236"
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Image(filename= pathFolder + 'misc_images/German_Shepherd.jpg')\n",
    "# Image(filename= pathFolder + 'misc_images/blue_tit.jpg')\n",
    "Image(filename= pathFolder + 'misc_images/avocado_2.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LkYlDMku-V7T"
   },
   "source": [
    "-----------\n",
    "\n",
    "# **1. Problem Statement**\n",
    "\n",
    "The task of this Lab is to design a network for classifying and extracting features for three different categories:  dogs, birds, and fruits. \n",
    "\n",
    "Building a new network for each problem is extremely time consuming:\n",
    "- It needs a lot of effort to optimize the architecture of a network\n",
    "- For different datasets (e.g. size and type of data) we might need different architectures\n",
    "- It is difficult to train a network with small datasets\n",
    "\n",
    "Remedy: Transfer Learning and the use of pre-trained models:\n",
    "- Here we use MobileNet which is trained in 1000 categories with 1000 images per category.\n",
    "- This pre-trained model classifies 1000 categories pretty well. But, what if we are interested in some new categories? \n",
    "- Pre-trained Mobilenet classifies images and extracts features quite well. In some classes performs better than others. But, what if we are interested in the others? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r0hwLVI4lBSa"
   },
   "source": [
    "\n",
    "## **1.1 Performance of pre-trained MobileNet in  different categories.**\n",
    "\n",
    "Inspect the performance of the pre-trained mobileNet in new data. It shows different performance for different categories.\n",
    "\n",
    "Loading the pre-trained MobileNet network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9946,
     "status": "ok",
     "timestamp": 1579709500372,
     "user": {
      "displayName": "Alexandra Dumitriu",
      "photoUrl": "",
      "userId": "05284431433817975591"
     },
     "user_tz": 300
    },
    "id": "ZfMEvASn0_Gq",
    "outputId": "594b862c-f605-4f6b-c647-4b726fda0c4e"
   },
   "outputs": [],
   "source": [
    "# Choose the weights pretrained in the imagenet dataset\n",
    "import tensorflow as tf\n",
    "\n",
    "mobile = tf.keras.applications.mobilenet.MobileNet(weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T-7tQVF6vOSw"
   },
   "source": [
    "Inspect the Mobilenet and define the input shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 318,
     "status": "ok",
     "timestamp": 1579709506122,
     "user": {
      "displayName": "Alexandra Dumitriu",
      "photoUrl": "",
      "userId": "05284431433817975591"
     },
     "user_tz": 300
    },
    "id": "afTmlXhRdzaP",
    "outputId": "6f641cd7-2dc7-4ecb-d75c-1254d88f9117"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mobilenet_1.00_224\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)    (None, 225, 225, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 112, 112, 32)      864       \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv1_relu (ReLU)            (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)  (None, 112, 112, 32)      288       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormaliza (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_relu (ReLU)        (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_pw_1 (Conv2D)           (None, 112, 112, 64)      2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormaliza (None, 112, 112, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv_pw_1_relu (ReLU)        (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv_pad_2 (ZeroPadding2D)   (None, 113, 113, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)  (None, 56, 56, 64)        576       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormaliza (None, 56, 56, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_relu (ReLU)        (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_2 (Conv2D)           (None, 56, 56, 128)       8192      \n",
      "_________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_2_relu (ReLU)        (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)  (None, 56, 56, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_relu (ReLU)        (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_3 (Conv2D)           (None, 56, 56, 128)       16384     \n",
      "_________________________________________________________________\n",
      "conv_pw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_3_relu (ReLU)        (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_4 (ZeroPadding2D)   (None, 57, 57, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)  (None, 28, 28, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_4_bn (BatchNormaliza (None, 28, 28, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_relu (ReLU)        (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_4 (Conv2D)           (None, 28, 28, 256)       32768     \n",
      "_________________________________________________________________\n",
      "conv_pw_4_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_4_relu (ReLU)        (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)  (None, 28, 28, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_relu (ReLU)        (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_5 (Conv2D)           (None, 28, 28, 256)       65536     \n",
      "_________________________________________________________________\n",
      "conv_pw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_5_relu (ReLU)        (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_6 (ZeroPadding2D)   (None, 29, 29, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)  (None, 14, 14, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_bn (BatchNormaliza (None, 14, 14, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_relu (ReLU)        (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_6 (Conv2D)           (None, 14, 14, 512)       131072    \n",
      "_________________________________________________________________\n",
      "conv_pw_6_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_6_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_7 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_7_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_8 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_8_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_9 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_9_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_10 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_10_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_11 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_11_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_12 (ZeroPadding2D)  (None, 15, 15, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D) (None, 7, 7, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormaliz (None, 7, 7, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_relu (ReLU)       (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_12 (Conv2D)          (None, 7, 7, 1024)        524288    \n",
      "_________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_12_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D) (None, 7, 7, 1024)        9216      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_13 (Conv2D)          (None, 7, 7, 1024)        1048576   \n",
      "_________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_13_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_preds (Conv2D)          (None, 1, 1, 1000)        1025000   \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "act_softmax (Activation)     (None, 1000)              0         \n",
      "=================================================================\n",
      "Total params: 4,253,864\n",
      "Trainable params: 4,231,976\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mobile.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d9p0EdtiuvfY"
   },
   "outputs": [],
   "source": [
    "IMG_SIZE  = 224\n",
    "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZuXSp7KUAO2w"
   },
   "source": [
    "This function prepares the images for the MobileNet: input shape: (1, 224, 224, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SS9pn-dZ4gLg"
   },
   "outputs": [],
   "source": [
    "def prepare_image(img_path, img_size = IMG_SIZE):\n",
    "    img = image.load_img(img_path, target_size=(img_size, img_size))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array_expanded_dims = np.expand_dims(img_array, axis=0)\n",
    "    return keras.applications.mobilenet.preprocess_input(img_array_expanded_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0tXnW5LJAZ-c"
   },
   "source": [
    "Let's make some predictions (classification) by using the pre-trained MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7480,
     "status": "ok",
     "timestamp": 1579709569375,
     "user": {
      "displayName": "Alexandra Dumitriu",
      "photoUrl": "",
      "userId": "05284431433817975591"
     },
     "user_tz": 300
    },
    "id": "Tjohd1ttAWw0",
    "outputId": "39b9a72a-aed9-4421-a529-76ec11d5fbd2"
   },
   "outputs": [],
   "source": [
    "imagePath = pathFolder + 'misc_images/German_Shepherd.jpg'\n",
    "preprocessed_image = prepare_image(imagePath)\n",
    "\n",
    "# check the size that fits with MobilesNets\n",
    "print('Image shape: ',(preprocessed_image).shape,'\\n \\n Prediction:\\n ')\n",
    "\n",
    "# Use mobileNet to classify the image\n",
    "predictions = mobile.predict(preprocessed_image)\n",
    "results = imagenet_utils.decode_predictions(predictions)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GdwV9CVOi1bh"
   },
   "source": [
    "It is convenient to define a function to do the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lNl38cWli0ge"
   },
   "outputs": [],
   "source": [
    "def myClassifier(imagePath, pathFolder=pathFolder, mobile=mobile):\n",
    "    imagePathFull = pathFolder + imagePath\n",
    "    preprocessed_image = prepare_image(imagePathFull)\n",
    "    # Use mobileNet to classify the image\n",
    "    predictions = mobile.predict(preprocessed_image)\n",
    "    results = imagenet_utils.decode_predictions(predictions)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 915,
     "status": "ok",
     "timestamp": 1579709578169,
     "user": {
      "displayName": "Alexandra Dumitriu",
      "photoUrl": "",
      "userId": "05284431433817975591"
     },
     "user_tz": 300
    },
    "id": "37av15HPjYjx",
    "outputId": "1a3bb0cb-d794-477c-b3e5-02aa515396f6"
   },
   "outputs": [],
   "source": [
    "myClassifier('misc_images/labrador1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 683,
     "status": "ok",
     "timestamp": 1579709581347,
     "user": {
      "displayName": "Alexandra Dumitriu",
      "photoUrl": "",
      "userId": "05284431433817975591"
     },
     "user_tz": 300
    },
    "id": "-WNtGQpPA_Lc",
    "outputId": "6a931553-1075-4873-a146-612386b07060"
   },
   "outputs": [],
   "source": [
    "myClassifier('misc_images/poodle1.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gIt1Pbn9AZNo"
   },
   "source": [
    "So, the pre-trained MobileNet classifies very well the **breed of dogs**.\n",
    "\n",
    "Let's try now to classify the **breed of birds**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 512,
     "status": "ok",
     "timestamp": 1579709595675,
     "user": {
      "displayName": "Alexandra Dumitriu",
      "photoUrl": "",
      "userId": "05284431433817975591"
     },
     "user_tz": 300
    },
    "id": "0aW0c8atBPqa",
    "outputId": "7120cc8c-5e7a-4905-a7d2-a53d62382f7b"
   },
   "outputs": [],
   "source": [
    "myClassifier('misc_images/blue_tit_1.jpg')\n",
    "# myClassifier('misc_images/blue_tit_2.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2191,
     "status": "ok",
     "timestamp": 1579709603255,
     "user": {
      "displayName": "Alexandra Dumitriu",
      "photoUrl": "",
      "userId": "05284431433817975591"
     },
     "user_tz": 300
    },
    "id": "LK9R9NtvBeE5",
    "outputId": "7e17c125-53b7-4781-dfd6-69694027bcc8"
   },
   "outputs": [],
   "source": [
    "myClassifier('misc_images/crow_1.jpg')\n",
    "# myClassifier('misc_images/crow_2.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1004,
     "status": "ok",
     "timestamp": 1579709609001,
     "user": {
      "displayName": "Alexandra Dumitriu",
      "photoUrl": "",
      "userId": "05284431433817975591"
     },
     "user_tz": 300
    },
    "id": "4VLGqP9XS1ru",
    "outputId": "5298ede4-c803-4d79-acc9-ed896ea022f0"
   },
   "outputs": [],
   "source": [
    "myClassifier('misc_images/hawk_1.jpg')\n",
    "# myClassifier('misc_images/hawk_2.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "etglIqgLz_9j"
   },
   "source": [
    "What about **fruits**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 616,
     "status": "ok",
     "timestamp": 1579709616651,
     "user": {
      "displayName": "Alexandra Dumitriu",
      "photoUrl": "",
      "userId": "05284431433817975591"
     },
     "user_tz": 300
    },
    "id": "OzIEX5L_xyZZ",
    "outputId": "75440c89-cc78-4d05-bbd7-a5550575c3a8"
   },
   "outputs": [],
   "source": [
    "myClassifier('misc_images/mango_1.jpg')\n",
    "#myClassifier('misc_images/mango_2.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CEe-gT1iNrNE"
   },
   "outputs": [],
   "source": [
    "# Image(filename= pathFolder + 'misc_images/avocado_2.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 671,
     "status": "ok",
     "timestamp": 1579709634581,
     "user": {
      "displayName": "Alexandra Dumitriu",
      "photoUrl": "",
      "userId": "05284431433817975591"
     },
     "user_tz": 300
    },
    "id": "UdItVUCyzg7y",
    "outputId": "6dee79b7-1935-4e84-8884-7f9e81027213"
   },
   "outputs": [],
   "source": [
    "myClassifier('misc_images/avocado_1.jpg')\n",
    "# myClassifier('misc_images/avocado_2.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 896,
     "status": "ok",
     "timestamp": 1579709641966,
     "user": {
      "displayName": "Alexandra Dumitriu",
      "photoUrl": "",
      "userId": "05284431433817975591"
     },
     "user_tz": 300
    },
    "id": "CVTMbHMJ0ZIJ",
    "outputId": "d2ffe55a-0045-47ba-dea6-ba11ce973d9e"
   },
   "outputs": [],
   "source": [
    "# myClassifier('misc_images/coconuts_1.jpg')\n",
    "myClassifier('misc_images/coconuts_2.jpg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DRc73VWzVTSZ"
   },
   "source": [
    "### **How can we improve the mobileNet ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uDxwrcmpVdam"
   },
   "source": [
    "The pre-trained MobileNet architecture shows quite good performance. Nevertheless, we want to improve  its classification ability  on the classes of birds and fruits. Two possible ways to do that are:\n",
    "\n",
    "\n",
    "1.  Train the mobileNet from the scratch by using only images  for dogs, birds and fruits\n",
    "\n",
    "  - Training the MobileNet from the scratch will be overfitting because our dataset is too small and the network very expressive. \n",
    "\n",
    "2.   Use the pre-trained model a re-train just a few layers (fast and efficient).\n",
    "This is  a simple way to apply Transfer Learning methodology. \n",
    "\n",
    "  *  The convolution layers of the MobileNet can extract  abstract features from the images.  We can use (transfer) this knowledge by  keeping those pre-trained layers. \n",
    "  * We can adjust and train a few new layers to classify the new data from the extracted features\n",
    "\n",
    "\n",
    "\n",
    "<!-- As example,  we want to classify different breed of birds and fruits where the pre-trained mobileNet fails. \n",
    "* First, let's explore that the MobileNet indeed extract and learn abstract features\n",
    "* Then add a new dense fully connected layer (classifier) and train it. Try to add more dense layers.\n",
    "* Check the classification performance on the new task. Is it better than before?  -->\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fOcIziz0xyIE"
   },
   "source": [
    "## **1.3 Train the mobileNet from the scratch (`mobile_from_scratch`)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lnu71yqp28OL"
   },
   "source": [
    "Load the data into the ImageDataGenerator\n",
    "\n",
    "We use data augmentation:\n",
    "\n",
    " https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2309,
     "status": "ok",
     "timestamp": 1579709654153,
     "user": {
      "displayName": "Alexandra Dumitriu",
      "photoUrl": "",
      "userId": "05284431433817975591"
     },
     "user_tz": 300
    },
    "id": "O6ginvzU2wfL",
    "outputId": "68255491-9a35-4019-fa04-8ae2c7f11627"
   },
   "outputs": [],
   "source": [
    "train_datagen=ImageDataGenerator(preprocessing_function=preprocess_input,\n",
    "                                                  horizontal_flip=True, \n",
    "                                                  rotation_range=45, \n",
    "                                                  zoom_range=[0.8,1.0]) \n",
    "\n",
    "test_datagen=ImageDataGenerator(preprocessing_function=preprocess_input) \n",
    "\n",
    "\n",
    "# Count the number of groups and make an array with their names\n",
    "\n",
    "# TRAINING set\n",
    "pathTrain = pathFolder + 'trainData/'\n",
    "listGroupsTrain = os.listdir(pathTrain) # the directory path\n",
    "\n",
    "# TESTING set\n",
    "pathTest = pathFolder + 'testData/'\n",
    "listGroupsTest = os.listdir(pathTest) # the directory path\n",
    "\n",
    "# Load the data into the ImageDataGenerator\n",
    "train_generator=train_datagen.flow_from_directory(pathFolder+'trainData',\n",
    "                                                 target_size=(224,224),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=64,\n",
    "                                                 class_mode='categorical',\n",
    "                                                 shuffle=True, \n",
    "                                                 classes=listGroupsTrain)\n",
    "                                                 \n",
    "test_generator=test_datagen.flow_from_directory(pathFolder+'testData',\n",
    "                                                 target_size=(224,224),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=64,\n",
    "                                                 class_mode='categorical',\n",
    "                                                 shuffle=False, \n",
    "                                                 classes=listGroupsTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bmXFjIJr3B1B"
   },
   "source": [
    "**Load and train the un-trained MobileNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6ueNW4c7xxUe"
   },
   "outputs": [],
   "source": [
    "# Loading the un-trained weights model\n",
    "mobile_from_scratch = tf.keras.applications.mobilenet.MobileNet(input_shape=IMG_SHAPE, weights=None, \n",
    "                                                                classes=len(listGroupsTrain))\n",
    "\n",
    "# Compile the model with a ADAM optimizers and a Crossentropy loss function\n",
    "# mobile_from_scratch.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])      \n",
    "lrate = 0.001\n",
    "mobile_from_scratch.compile(optimizer=tf.keras.optimizers.Adam(lr=lrate),loss='categorical_crossentropy',metrics=['accuracy'])      \n",
    "\n",
    "\n",
    "step_size_train=train_generator.n//train_generator.batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 167007,
     "status": "ok",
     "timestamp": 1579709834115,
     "user": {
      "displayName": "Alexandra Dumitriu",
      "photoUrl": "",
      "userId": "05284431433817975591"
     },
     "user_tz": 300
    },
    "id": "79E7Whb_ZOY_",
    "outputId": "e05e46ae-8c80-411f-ea9e-7d1bea1f7b21"
   },
   "outputs": [],
   "source": [
    "# Train the model and save the loss and accuracy of the training\n",
    "# and validation data in the history variable\n",
    "historyModel_scratch = mobile_from_scratch.fit_generator(generator=train_generator,\n",
    "                                                         validation_data=test_generator,\n",
    "                                                         steps_per_epoch=step_size_train,\n",
    "                                                         epochs=10)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IJgrf-ecJ_1G"
   },
   "outputs": [],
   "source": [
    "def plot_history(histories, titles, clrs='red'):\n",
    "  # helper function to plot the loss and accuracy \n",
    "  plt.figure(figsize=(18, 7))\n",
    "  # for history, title in zip(histories,titles):\n",
    "  for history, title, clr in zip(histories,titles,clrs):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "# color='green', marker='o', linestyle='dashed'\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, len(acc)+1), acc, color=clr, label='Training Accuracy: ' + title)\n",
    "    plt.plot(range(1, len(acc)+1), val_acc, color=clr, linestyle='dashed', label='Validation Accuracy: ' + title)\n",
    "    plt.legend()\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([min(plt.ylim()),1])\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, len(acc)+1), loss, color=clr,  label='Training Loss: ' + title)\n",
    "    plt.plot(range(1, len(acc)+1), val_loss, color=clr, linestyle='dashed', label='Validation Loss: ' + title)\n",
    "    plt.legend()\n",
    "    plt.ylabel('Cross Entropy')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('epoch')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vgyg02P13voV"
   },
   "source": [
    "**Plot our training and validation loss and accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 948,
     "status": "ok",
     "timestamp": 1579709844977,
     "user": {
      "displayName": "Alexandra Dumitriu",
      "photoUrl": "",
      "userId": "05284431433817975591"
     },
     "user_tz": 300
    },
    "id": "p8RTdfzvOMzZ",
    "outputId": "f36cd311-63f2-4e46-bec0-9ca4d24822a4"
   },
   "outputs": [],
   "source": [
    "histories = [historyModel_scratch]\n",
    "titles = ['model from scratch']\n",
    "clr = ['green']\n",
    "\n",
    "plot_history(histories, titles, clr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-B0MEp2HuLyM"
   },
   "source": [
    "We are overfitting the MobileNet very fast. It makes sense, the network is too expressive and the dataset too small. \n",
    "\n",
    "So, how can we take an advantage of this efficient network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6FnWLeLXlnBK"
   },
   "source": [
    "---------------\n",
    "\n",
    "\n",
    "# **2. Transfer Learning to the rescue**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Cm14FKphuVV"
   },
   "source": [
    "## **2.1 Explore the pre-trained mobileNet features**\n",
    "\n",
    " MobileNet is able to extract abstract features from images. Inpsecting the network will figure out which *transfer values* we need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JoaTnWskXTg6"
   },
   "source": [
    "Let's extract the features that MobileNet learns for three  images, one from each category of dogs, birds, and fruits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2fwtU8J-XRYI"
   },
   "outputs": [],
   "source": [
    "preprocessed_image1 = prepare_image(pathFolder + 'misc_images/German_Shepherd.jpg')\n",
    "preprocessed_image2 = prepare_image(pathFolder + 'misc_images/hawk_1.jpg')\n",
    "preprocessed_image3 = prepare_image(pathFolder + 'misc_images/mango_1.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ojbDelyZELDq"
   },
   "source": [
    "Inspect the first convolution layer: `conv1`\n",
    "\n",
    "Load only up to the first convolution layer. We need to know the name of the layer (check with `*.summary()`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 554
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3254,
     "status": "ok",
     "timestamp": 1579709863768,
     "user": {
      "displayName": "Alexandra Dumitriu",
      "photoUrl": "",
      "userId": "05284431433817975591"
     },
     "user_tz": 300
    },
    "id": "q47PvMgjEXRP",
    "outputId": "eb357d48-a517-497c-d4a5-b4df039a0f17"
   },
   "outputs": [],
   "source": [
    "# load the model up to first convolution layers\n",
    "inspectLayerModel = tf.keras.models.Model(inputs=mobile.input, outputs=mobile.get_layer('conv1').output)\n",
    "\n",
    "features1 = inspectLayerModel.predict(preprocessed_image1)\n",
    "features2 = inspectLayerModel.predict(preprocessed_image2)\n",
    "features3 = inspectLayerModel.predict(preprocessed_image3)\n",
    "\n",
    "print('shape: ', features1.shape , '\\n')\n",
    "print('type: ',type(features1), '\\n')\n",
    "\n",
    "# Plot the first N features for each image\n",
    "N=5\n",
    "plt.figure(figsize=[16,8])\n",
    "for i in range(N):\n",
    "    # i in a different feature of each imge\n",
    "    plt.subplot(3,N,i+1)\n",
    "    plt.imshow(features1[0,:,:,i])\n",
    "    plt.subplot(3,N,N+i+1)\n",
    "    plt.imshow(features2[0,:,:,i])\n",
    "    plt.subplot(3,N,2*N + i+1)\n",
    "    plt.imshow(features3[0,:,:,i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "75fPaQysHWeK"
   },
   "source": [
    "Inspect one layer deeper: `conv_pw_1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3670,
     "status": "ok",
     "timestamp": 1579709874705,
     "user": {
      "displayName": "Alexandra Dumitriu",
      "photoUrl": "",
      "userId": "05284431433817975591"
     },
     "user_tz": 300
    },
    "id": "QL5bIijFHW1K",
    "outputId": "17737756-bebc-4069-a31a-3ff087b93312"
   },
   "outputs": [],
   "source": [
    "inspectLayerModel = tf.keras.models.Model(inputs=mobile.input, outputs=mobile.get_layer('conv_pw_1').output)\n",
    "\n",
    "features1 = inspectLayerModel.predict(preprocessed_image1)\n",
    "features2 = inspectLayerModel.predict(preprocessed_image2)\n",
    "features3 = inspectLayerModel.predict(preprocessed_image3)\n",
    "\n",
    "print('shape: ', features1.shape , '\\n')\n",
    "\n",
    "plt.figure(figsize=[16,8])\n",
    "for i in range(N):\n",
    "    plt.subplot(3,N,i+1)\n",
    "    plt.imshow(features1[0,:,:,i])\n",
    "    plt.subplot(3,N,N+i+1)\n",
    "    plt.imshow(features2[0,:,:,i])\n",
    "    plt.subplot(3,N,2*N + i+1)\n",
    "    plt.imshow(features3[0,:,:,i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jxQO6VhXGJg-"
   },
   "source": [
    "Inpsect the last convolution layer: `conv_pw_13`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2942,
     "status": "ok",
     "timestamp": 1579709886609,
     "user": {
      "displayName": "Alexandra Dumitriu",
      "photoUrl": "",
      "userId": "05284431433817975591"
     },
     "user_tz": 300
    },
    "id": "17QbcH1BGXsq",
    "outputId": "16445529-f685-4d0a-e3f5-3d91c683afe0"
   },
   "outputs": [],
   "source": [
    "inspectLayerModel = tf.keras.models.Model(inputs=mobile.input, outputs=mobile.get_layer('conv_pw_13_relu').output)\n",
    "\n",
    "features1 = inspectLayerModel.predict(preprocessed_image1)\n",
    "features2 = inspectLayerModel.predict(preprocessed_image2)\n",
    "features3 = inspectLayerModel.predict(preprocessed_image3)\n",
    "\n",
    "print('shape: ', features1.shape, '\\n')\n",
    "\n",
    "plt.figure(figsize=[16,8])\n",
    "for i in range(N):\n",
    "    plt.subplot(3,N,i+1)\n",
    "    plt.imshow(features1[0,:,:,i])\n",
    "    plt.subplot(3,N,N+i+1)\n",
    "    plt.imshow(features2[0,:,:,i])\n",
    "    plt.subplot(3,N,2*N + i+1)\n",
    "    plt.imshow(features3[0,:,:,i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Etjyg3bk6zNc"
   },
   "source": [
    "Totally abstract features. Let's check for similarities between similar pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2488,
     "status": "ok",
     "timestamp": 1579709898654,
     "user": {
      "displayName": "Alexandra Dumitriu",
      "photoUrl": "",
      "userId": "05284431433817975591"
     },
     "user_tz": 300
    },
    "id": "eFFPlarL6xjG",
    "outputId": "32aef97b-68af-4ec2-97be-1869cf6d8aae"
   },
   "outputs": [],
   "source": [
    "preprocessed_image1b = prepare_image(pathFolder + 'misc_images/labrador1.jpg')\n",
    "preprocessed_image2b = prepare_image(pathFolder + 'misc_images/hawk_2.jpg')\n",
    "preprocessed_image3b = prepare_image(pathFolder + 'misc_images/mango_2.jpg')\n",
    "\n",
    "features1 = inspectLayerModel.predict(preprocessed_image1)\n",
    "features1b = inspectLayerModel.predict(preprocessed_image1b)\n",
    "features2 = inspectLayerModel.predict(preprocessed_image2)\n",
    "features2b = inspectLayerModel.predict(preprocessed_image2b)\n",
    "features3 = inspectLayerModel.predict(preprocessed_image3)\n",
    "features3b = inspectLayerModel.predict(preprocessed_image3b)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=[16,8])\n",
    "for i in range(N):\n",
    "    plt.subplot(2,N,i+1)\n",
    "    plt.imshow(features2[0,:,:,i])\n",
    "    plt.subplot(2,N,N+i+1)\n",
    "    plt.imshow(features2b[0,:,:,i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lCXSpl8XbjxQ"
   },
   "source": [
    "\n",
    "The features seem  meaningless,  but is that true?\n",
    "\n",
    "Let's explore those features by using a dimensional reduction method such as Principle Component Analysis (PCA). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kVETnoDsyUtT"
   },
   "source": [
    "**PCA for further analysis of transfer values:**\n",
    "\n",
    "To save some time we use only 20 images per category. These data are store at `pcaData`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A3MI0RCLgbY_"
   },
   "source": [
    "Define the layer from which we will extract the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3275,
     "status": "ok",
     "timestamp": 1579709913132,
     "user": {
      "displayName": "Alexandra Dumitriu",
      "photoUrl": "",
      "userId": "05284431433817975591"
     },
     "user_tz": 300
    },
    "id": "RTHI3h-egiAL",
    "outputId": "a1296285-5ecc-4edd-b8bf-6e15b1155602"
   },
   "outputs": [],
   "source": [
    "# featuresLayerModel = tf.keras.models.Model(inputs=mobile.input, outputs=mobile.get_layer('conv_pw_13_relu').output)\n",
    "\n",
    "# ALTERNATIVELY\n",
    "\n",
    "featuresLayerModel = tf.keras.applications.MobileNet(input_shape=IMG_SHAPE, weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vfoAqEYegpL5"
   },
   "source": [
    "Count the images in the PCA dataset and define an array with the associate name of category of each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MRFiQWfzgnw3"
   },
   "outputs": [],
   "source": [
    "# location\n",
    "pathPCA = pathFolder + 'pcaData/'\n",
    "\n",
    "# groups\n",
    "listGroups = os.listdir(pathPCA) # the directory path\n",
    "number_groups = len(listGroups)\n",
    "\n",
    "# count the pictures\n",
    "totalImages = 0\n",
    "for pathF in listGroups:\n",
    "    pathF1 = pathPCA + pathF\n",
    "    listFiles = os.listdir(pathF1) \n",
    "    totalImages += len(listFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-DfR8wc1qSAo"
   },
   "outputs": [],
   "source": [
    "# PCA take a 1D object per image\n",
    "features_flatten = np.zeros([totalImages,7*7*1024]) \n",
    "\n",
    "i=0\n",
    "for pathF in listGroups:\n",
    "    pathF1 = pathPCA + pathF + \"/\"\n",
    "    for imgName in os.listdir(pathF1):\n",
    "        pathImag = pathF1 + imgName\n",
    "        preprocessed_img = prepare_image(pathImag)\n",
    "        features = featuresLayerModel.predict(preprocessed_img)\n",
    "        features_flatten[i,:] = features.flatten()\n",
    "        i += 1        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EMdmbFUJz6hp"
   },
   "source": [
    "Perform a PCA transform in the extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mceoMQ0vwjGe"
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "transferValues = features_flatten\n",
    "transferVal_pca = pca.fit_transform(transferValues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QX71FgAX84ne"
   },
   "source": [
    "Plot the features after PCA transform and dimensionsal reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 592
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 818,
     "status": "ok",
     "timestamp": 1579710255075,
     "user": {
      "displayName": "Alexandra Dumitriu",
      "photoUrl": "",
      "userId": "05284431433817975591"
     },
     "user_tz": 300
    },
    "id": "_86UB8c94X7m",
    "outputId": "1889c6f2-af10-420b-ea71-034373fd115b"
   },
   "outputs": [],
   "source": [
    "x = transferVal_pca[:, 0]\n",
    "y = transferVal_pca[:, 1]\n",
    "\n",
    "mystyle = ['ok', 'or','^m','^y','*r','*k', '*b','ob']\n",
    "\n",
    "plt.figure(figsize=[14,10])\n",
    "\n",
    "# The number of images per category in the pca dataset \n",
    "imgC = 20\n",
    "\n",
    "for i in range(number_groups):\n",
    "    plt.plot(x[i*imgC : imgC*(i+1)],y[i*imgC : imgC*(i+1)], mystyle[i], label=listGroups[i] )\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hK8NicKShuya"
   },
   "source": [
    " ## **2.2 Add and train  a new dense layer on the top of the pre-trained MobileNet (`mobile_1_layer`)**\n",
    "\n",
    "\n",
    "The plan is to use the pre-trained convolution layers obtained by MobileNet (base model), adjust a new dense layer in the end (classifier). Then freeze the base layers and train only the classifier.\n",
    "\n",
    "We are interested only in 8 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 472,
     "status": "ok",
     "timestamp": 1579710665351,
     "user": {
      "displayName": "Alexandra Dumitriu",
      "photoUrl": "",
      "userId": "05284431433817975591"
     },
     "user_tz": 300
    },
    "id": "xCPXMfcevIxm",
    "outputId": "baff7f38-b641-4566-88fc-de5598cd6685"
   },
   "outputs": [],
   "source": [
    "# Count the number of groups and make an array with their names\n",
    "pathTrain = pathFolder + 'trainData/'\n",
    "listGroupsTrain = os.listdir(pathTrain) # the directory path\n",
    "\n",
    "pathTest = pathFolder + 'testData/'\n",
    "listGroupsTest = os.listdir(pathTest) # the directory path\n",
    "\n",
    "number_groupsTrain = len(listGroupsTrain)\n",
    "print(number_groupsTrain, 'new groups')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gwfkEO1nWwwN"
   },
   "source": [
    "Define the **base model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XZy_wMQOQe2d"
   },
   "outputs": [],
   "source": [
    "transferModel_base = tf.keras.applications.MobileNet(input_shape=IMG_SHAPE, weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qRQBpB5z7gg5"
   },
   "outputs": [],
   "source": [
    "# transferModel_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7FGywkPW0Y8"
   },
   "source": [
    "Create the new architecture by transfering the convolutions layers from the mobilenet. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1302,
     "status": "ok",
     "timestamp": 1579710681783,
     "user": {
      "displayName": "Alexandra Dumitriu",
      "photoUrl": "",
      "userId": "05284431433817975591"
     },
     "user_tz": 300
    },
    "id": "MwxMFft27fcF",
    "outputId": "3f60f185-49e3-4d02-9d4e-12ade15cf6e9"
   },
   "outputs": [],
   "source": [
    "# Freeze the mobileNet's layers\n",
    "transferModel_base.trainable = False\n",
    "\n",
    "# Regularize the dense layer by using L1\n",
    "kernel_weight = 0.02\n",
    "bias_weight = 0.02\n",
    "\n",
    "mobile_1_layer = tf.keras.Sequential([\n",
    "      transferModel_base,\n",
    "      tf.keras.layers.GlobalAveragePooling2D(),\n",
    "## You might want to add an extra dense layer\n",
    "      # Dense(32, activation='relu',\n",
    "      #                        kernel_regularizer=tf.keras.regularizers.l1(kernel_weight), \n",
    "      #                        bias_regularizer=tf.keras.regularizers.l1(bias_weight)),\n",
    "      Dense(number_groupsTrain, activation='softmax', name=\"dense_head\",\n",
    "                                 kernel_regularizer=tf.keras.regularizers.l1(kernel_weight), \n",
    "                                 bias_regularizer=tf.keras.regularizers.l1(bias_weight))\n",
    "])\n",
    "\n",
    "\n",
    "mobile_1_layer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N_1Ink_6ZQjd"
   },
   "source": [
    "Compile the model: Define optimizer, loss function, and the number of training period (epochs). \n",
    "\n",
    "It  takes a few minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xhIPq-YPp0yb"
   },
   "outputs": [],
   "source": [
    "# Adam optimizer, loss function will be categorical cross entropy\n",
    "lrate = 0.002\n",
    "mobile_1_layer.compile(optimizer=tf.keras.optimizers.Adam(lr=lrate),\n",
    "                     loss='categorical_crossentropy' ,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 68225,
     "status": "ok",
     "timestamp": 1579710879956,
     "user": {
      "displayName": "Alexandra Dumitriu",
      "photoUrl": "",
      "userId": "05284431433817975591"
     },
     "user_tz": 300
    },
    "id": "n9MpW3t9Y3gM",
    "outputId": "b3d5134a-c8c5-467f-a7c7-59f850413471"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-478422830e4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstep_size_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m history_mobile_1_layer = mobile_1_layer.fit_generator(generator=train_generator,\n\u001b[1;32m      4\u001b[0m                                        \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                        \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep_size_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_generator' is not defined"
     ]
    }
   ],
   "source": [
    "step_size_train=train_generator.n//train_generator.batch_size\n",
    "\n",
    "history_mobile_1_layer = mobile_1_layer.fit_generator(generator=train_generator,\n",
    "                                       validation_data=test_generator,\n",
    "                                       steps_per_epoch=step_size_train,\n",
    "                                       epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fW0dwqoU37Yt"
   },
   "source": [
    "**Check the performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1266,
     "status": "ok",
     "timestamp": 1579710902958,
     "user": {
      "displayName": "Alexandra Dumitriu",
      "photoUrl": "",
      "userId": "05284431433817975591"
     },
     "user_tz": 300
    },
    "id": "d15n9yqPu0kT",
    "outputId": "f2ea4cef-3eea-412a-eea7-3f8863bcb3ec"
   },
   "outputs": [],
   "source": [
    "histories = [historyModel_scratch, history_mobile_1_layer]\n",
    "titles = ['model from scratch', '1 layer re-trained']\n",
    "clrs = ['green', 'red']\n",
    "plot_history(histories, titles, clrs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "irTVlslLteYC"
   },
   "source": [
    "##  **2.3 Re-train the last-2 layers of pre-trained mobileNet (`mobile_2_layers`)**\n",
    "\n",
    "During the training of the previoius new model we had frozen the pre-trained layers got by mobileNet. So, by doing PCA in the features (as we did earlier) we will not see any change. We can improve the features extracted by also training the last convolution layer of the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AoFM_ggj_k1a"
   },
   "outputs": [],
   "source": [
    "transferModel_base_fine_tuned_2_layers = tf.keras.applications.MobileNet(input_shape=IMG_SHAPE, weights='imagenet', include_top=False)\n",
    "\n",
    "transferModel_base_fine_tuned_2_layers.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 408,
     "status": "ok",
     "timestamp": 1579711253255,
     "user": {
      "displayName": "Alexandra Dumitriu",
      "photoUrl": "",
      "userId": "05284431433817975591"
     },
     "user_tz": 300
    },
    "id": "zjA9U-QFBrEo",
    "outputId": "b22c030b-8c69-4936-f650-bd9129aa1295"
   },
   "outputs": [],
   "source": [
    "# The number of the layers in the base model\n",
    "NumOfLayers = len(transferModel_base_fine_tuned_2_layers.layers)\n",
    "print(\"Number of layers in the base model: \", NumOfLayers)\n",
    "\n",
    "# Earlier we froze all the layers by: transferModel_base.trainable = False\n",
    "# Print out  the name of the layers to  decide which to un-freeze\n",
    "i = 1\n",
    "for layer in transferModel_base_fine_tuned_2_layers.layers[:NumOfLayers]:\n",
    "    print(i,layer.name)\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1246,
     "status": "ok",
     "timestamp": 1579711293889,
     "user": {
      "displayName": "Alexandra Dumitriu",
      "photoUrl": "",
      "userId": "05284431433817975591"
     },
     "user_tz": 300
    },
    "id": "_rrwg03g495c",
    "outputId": "7e6b49ec-98d2-4d37-b0f5-95eafa38f28c"
   },
   "outputs": [],
   "source": [
    "# Fine-tune only the last convolution: (be careful, the last is  relu, we don;t need that)\n",
    "i = NumOfLayers-3\n",
    "for layer in transferModel_base_fine_tuned_2_layers.layers[NumOfLayers-3:NumOfLayers]:\n",
    "    layer.trainable =  True\n",
    "    print(i,layer.name)\n",
    "    i+=1\n",
    "\n",
    "mobile_2_layers = tf.keras.Sequential([\n",
    "  transferModel_base_fine_tuned_2_layers,\n",
    "  tf.keras.layers.GlobalAveragePooling2D(),\n",
    "Dense(number_groupsTrain, activation='softmax',\n",
    "                           kernel_regularizer=tf.keras.regularizers.l1(kernel_weight), \n",
    "                           bias_regularizer=tf.keras.regularizers.l1(bias_weight))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 68088,
     "status": "ok",
     "timestamp": 1579711414188,
     "user": {
      "displayName": "Alexandra Dumitriu",
      "photoUrl": "",
      "userId": "05284431433817975591"
     },
     "user_tz": 300
    },
    "id": "pb1XdL4yBrVs",
    "outputId": "5e3001a7-4bc2-458a-976d-07786d6bbfd8"
   },
   "outputs": [],
   "source": [
    "mobile_2_layers.compile(optimizer=tf.keras.optimizers.Adam(lr=lrate),\n",
    "                     loss='categorical_crossentropy' ,metrics=['accuracy'])\n",
    "\n",
    "\n",
    "step_size_train=train_generator.n//train_generator.batch_size\n",
    "\n",
    "history_mobile_2_layers = mobile_2_layers.fit_generator(generator=train_generator,\n",
    "                                                  validation_data=test_generator,\n",
    "                                                  steps_per_epoch=step_size_train,\n",
    "                                                  epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ho67kyd4rfjy"
   },
   "source": [
    "\n",
    "**Compare the performance for the 1 versus 2 layers re-training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1230,
     "status": "ok",
     "timestamp": 1579711420061,
     "user": {
      "displayName": "Alexandra Dumitriu",
      "photoUrl": "",
      "userId": "05284431433817975591"
     },
     "user_tz": 300
    },
    "id": "yuJ0vQIerPRd",
    "outputId": "f2607223-67fa-4bc9-fc8c-c0b5a9b391bf"
   },
   "outputs": [],
   "source": [
    "histories = [history_mobile_1_layer, history_mobile_2_layers]\n",
    "titles = ['1 layer re-trained', '2 layer re-trained']\n",
    "clrs = ['red', 'orange']\n",
    "\n",
    "plot_history(histories, titles, clrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bg5CKJYxrqLL"
   },
   "source": [
    "**Inspect what happens with the features**\n",
    "\n",
    "Define first a function to perform PCA transform and make the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WBA0MDdTChtY"
   },
   "outputs": [],
   "source": [
    "def plot_feature_extraction(transferModel_base):\n",
    "  featuresTunedModel = tf.keras.models.Model(inputs=transferModel_base.input,\n",
    "                                             outputs=transferModel_base.get_layer('conv_pw_13_relu').output)\n",
    "\n",
    "\n",
    "  # Count the number of groups and make an array with their names\n",
    "  pathPCA = pathFolder + 'pcaData/'\n",
    "   \n",
    "  listGroups = os.listdir(pathPCA) # the directory path\n",
    "  number_groups = len(listGroups)\n",
    "\n",
    "  totalImages = 0\n",
    "  for pathF in listGroups:\n",
    "      pathF1 = pathPCA + pathF\n",
    "      listFiles = os.listdir(pathF1) \n",
    "      totalImages += len(listFiles)\n",
    "\n",
    "\n",
    "  features_tuning = np.zeros([totalImages,7*7*1024]) # for using the conv_pw_13 layer\n",
    "\n",
    "  i=0\n",
    "  for pathF in listGroups:\n",
    "      pathF1 = pathPCA + pathF + \"/\"\n",
    "      for imgName in os.listdir(pathF1):\n",
    "          pathImag = pathF1 + imgName\n",
    "          preprocessed_img = prepare_image(pathImag)\n",
    "          features = featuresTunedModel.predict(preprocessed_img)\n",
    "\n",
    "          features_tuning[i,:] = features.flatten()\n",
    "          i += 1        \n",
    "\n",
    "  pcaT = PCA(n_components=2)\n",
    "  transferValues = features_tuning\n",
    "  transferVal_pcaT = pcaT.fit_transform(transferValues)\n",
    "\n",
    "  x = transferVal_pcaT[:, 0]\n",
    "  y = transferVal_pcaT[:, 1]\n",
    "\n",
    "  mystyle = ['ok', 'or','^m','^y','*r','*k', '*b','ob']\n",
    "\n",
    "  imgC = 20\n",
    "\n",
    "  plt.figure(figsize=[12,8])\n",
    "  for i in range(number_groups):\n",
    "    plt.plot(x[i*imgC : imgC*(i+1)],y[i*imgC : imgC*(i+1)], mystyle[i], label=listGroups[i] )\n",
    "  plt.legend();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 24687,
     "status": "ok",
     "timestamp": 1579711662484,
     "user": {
      "displayName": "Alexandra Dumitriu",
      "photoUrl": "",
      "userId": "05284431433817975591"
     },
     "user_tz": 300
    },
    "id": "0Qc1YMTDChkj",
    "outputId": "fab935a4-2a0c-47d5-d644-a8ce8cf815b2"
   },
   "outputs": [],
   "source": [
    "plot_feature_extraction(transferModel_base_fine_tuned_2_layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DS5T4AByunWk"
   },
   "source": [
    "Compared to previously when only the last dense layer was trained and the feature extraction convolutional layers where exactly the ones from the pre-trained Imagenet model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11263,
     "status": "ok",
     "timestamp": 1579711683558,
     "user": {
      "displayName": "Alexandra Dumitriu",
      "photoUrl": "",
      "userId": "05284431433817975591"
     },
     "user_tz": 300
    },
    "id": "Fqzkw5n7umbJ",
    "outputId": "2f5cb35a-1ef0-42b6-faec-4e84cff51415"
   },
   "outputs": [],
   "source": [
    "plot_feature_extraction(transferModel_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FKg3S-K_pPL-"
   },
   "source": [
    "## **2.4 Re-train all layers of pre-trained mobileNet (`mobile_all_layers`)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 72444,
     "status": "ok",
     "timestamp": 1579711768104,
     "user": {
      "displayName": "Alexandra Dumitriu",
      "photoUrl": "",
      "userId": "05284431433817975591"
     },
     "user_tz": 300
    },
    "id": "QrOzEefbpTm8",
    "outputId": "c2b88443-3fb7-4141-e593-fe95b10d9570"
   },
   "outputs": [],
   "source": [
    "transferModel_base_fine_tuned_all_layers = tf.keras.applications.MobileNet(input_shape=IMG_SHAPE, weights='imagenet', include_top=False)\n",
    "\n",
    "# Train the entire network\n",
    "transferModel_base_fine_tuned_all_layers.trainable = True\n",
    "\n",
    "\n",
    "mobile_all_layers = tf.keras.Sequential([\n",
    "  transferModel_base_fine_tuned_all_layers,\n",
    "  tf.keras.layers.GlobalAveragePooling2D(),\n",
    "  Dense(number_groupsTrain, activation='softmax',\n",
    "                         kernel_regularizer=tf.keras.regularizers.l1(kernel_weight), \n",
    "                          bias_regularizer=tf.keras.regularizers.l1(bias_weight))\n",
    "])\n",
    "\n",
    "\n",
    "mobile_all_layers.compile(optimizer=tf.keras.optimizers.Adam(lr=lrate),\n",
    "                     loss='categorical_crossentropy' ,metrics=['accuracy'])\n",
    "\n",
    "\n",
    "step_size_train=train_generator.n//train_generator.batch_size\n",
    "\n",
    "history_mobile_all_layers = mobile_all_layers.fit_generator(generator=train_generator,\n",
    "                                                  validation_data=test_generator,\n",
    "                                                  steps_per_epoch=step_size_train,\n",
    "                                                  epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DWSLKgA7vHTu"
   },
   "source": [
    "**Let's compare the loss and accuracy for the 1 versus 2 and versus all  layers re-training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1174,
     "status": "ok",
     "timestamp": 1579711772582,
     "user": {
      "displayName": "Alexandra Dumitriu",
      "photoUrl": "",
      "userId": "05284431433817975591"
     },
     "user_tz": 300
    },
    "id": "7cDGNo9ivHTx",
    "outputId": "efd63b87-81c5-4216-cca7-71f930f7467f"
   },
   "outputs": [],
   "source": [
    "histories = [history_mobile_1_layer, history_mobile_2_layers, history_mobile_all_layers]\n",
    "titles = ['1 layer re-trained', '2 layers re-trained', 'all layers re-trained' ]\n",
    "clrs = ['red', 'orange', 'black']\n",
    "plot_history(histories, titles, clrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IcQFvbw8vHT1"
   },
   "source": [
    "**Inspect what happened with the features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11331,
     "status": "ok",
     "timestamp": 1579711796186,
     "user": {
      "displayName": "Alexandra Dumitriu",
      "photoUrl": "",
      "userId": "05284431433817975591"
     },
     "user_tz": 300
    },
    "id": "ZrmoEUa9vHT3",
    "outputId": "c4e70f77-d605-47a7-d3e8-2979c50469aa"
   },
   "outputs": [],
   "source": [
    "plot_feature_extraction(transferModel_base_fine_tuned_all_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J0OIl61C8GY2"
   },
   "source": [
    "## **2.5 Fine-tuning**\n",
    "\n",
    "We use the first architecture where we  trained only the classifier layer. Now, we unfreeze the convolution layers and keep training the entire network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 338,
     "status": "ok",
     "timestamp": 1579711801586,
     "user": {
      "displayName": "Alexandra Dumitriu",
      "photoUrl": "",
      "userId": "05284431433817975591"
     },
     "user_tz": 300
    },
    "id": "JhJaTz1s2wzv",
    "outputId": "ee0c0686-1767-4914-9df2-e0f1ba520998"
   },
   "outputs": [],
   "source": [
    "mobile_1_layer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LAtlymYD4eJF"
   },
   "outputs": [],
   "source": [
    "# unfreeze the convolution layers\n",
    "transferModel_baseTune = tf.keras.applications.MobileNet(input_shape=IMG_SHAPE, weights='imagenet', include_top=False)\n",
    "transferModel_baseTune.trainable = True\n",
    "\n",
    "# re-define the architecture\n",
    "mobile_tuned = tf.keras.Sequential([\n",
    "  transferModel_baseTune,\n",
    "  tf.keras.layers.GlobalAveragePooling2D(),\n",
    "  mobile_1_layer.get_layer('dense_head')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 436,
     "status": "ok",
     "timestamp": 1579711825557,
     "user": {
      "displayName": "Alexandra Dumitriu",
      "photoUrl": "",
      "userId": "05284431433817975591"
     },
     "user_tz": 300
    },
    "id": "zMmzi-j-5YUe",
    "outputId": "3e2c638f-1ac8-474b-a0b6-e9851d6f4a54"
   },
   "outputs": [],
   "source": [
    "mobile_tuned.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 72065,
     "status": "ok",
     "timestamp": 1579711904730,
     "user": {
      "displayName": "Alexandra Dumitriu",
      "photoUrl": "",
      "userId": "05284431433817975591"
     },
     "user_tz": 300
    },
    "id": "2UPcgaHE5sMK",
    "outputId": "a808308d-3325-49c1-f3a4-ef28c76d244b"
   },
   "outputs": [],
   "source": [
    "mobile_tuned.compile(optimizer=tf.keras.optimizers.Adam(lr=lrate),\n",
    "                     loss='categorical_crossentropy' ,metrics=['accuracy'])\n",
    "\n",
    "\n",
    "step_size_train=train_generator.n//train_generator.batch_size\n",
    "\n",
    "history_mobile_tuned = mobile_tuned.fit_generator(generator=train_generator,\n",
    "                                                  validation_data=test_generator,\n",
    "                                                  steps_per_epoch=step_size_train,\n",
    "                                                  epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "078liu2L5tTS"
   },
   "source": [
    "Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1449,
     "status": "ok",
     "timestamp": 1579711907870,
     "user": {
      "displayName": "Alexandra Dumitriu",
      "photoUrl": "",
      "userId": "05284431433817975591"
     },
     "user_tz": 300
    },
    "id": "huCEpnSf6Y9R",
    "outputId": "5fcd6592-91c5-4f62-ec3b-180246f41b9a"
   },
   "outputs": [],
   "source": [
    "histories = [history_mobile_1_layer, history_mobile_2_layers, history_mobile_all_layers, history_mobile_tuned]\n",
    "titles = ['1 layer re-trained', '2 layers re-trained', 'all layers re-trained', 'tuning']\n",
    "clrs = ['red', 'orange', 'black', 'blue']\n",
    "\n",
    "plot_history(histories, titles, clrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wFiZ76KJ5vzB"
   },
   "source": [
    "**Features inspection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11696,
     "status": "ok",
     "timestamp": 1579711925378,
     "user": {
      "displayName": "Alexandra Dumitriu",
      "photoUrl": "",
      "userId": "05284431433817975591"
     },
     "user_tz": 300
    },
    "id": "OM1TyCQm6lUs",
    "outputId": "dee63fa3-3c34-48f3-a115-8a7f6ca5d58b"
   },
   "outputs": [],
   "source": [
    "plot_feature_extraction(transferModel_baseTune)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "miG76O5gzREO"
   },
   "source": [
    "## **Conclusion:**\n",
    "\n",
    "**Transfer learning  for classification:**\n",
    "\n",
    "Training a deep network (such as mobileNet, inception, VGG etc) from the scratch is time consuming. Designing a new network to classify a particular dataset requires a lot of effort (hyperparameter optimization etc). We might use a pre-trained deep network  and re-train only a few new layers. In particular, we can use the convolution layers that already have been trained to extract features and add on the top one or more dense layers, the classifier block. We freeze the convolution layers and train only the classifier block of layers on a certain dataset. *This proccess improves the classification but does not improve the feature extraction.*\n",
    "\n",
    "\n",
    "**Fine tuning for feature extraction:**\n",
    "\n",
    "When we are working with a small dataset it is difficult to train a network to extract features. On the other hand, a pre-trained model that has been trained on a big dataset is able to extract features much better. However, it might not perform very well in extracting features for a particular dataset. In that case we can improve the feature extraction by fine tuning. First, we  freeze  the pre-trained convolution layers and train the new classifier layers (dense). Once we have trained the classifier we unfreeze the convolution layers and keep training the entire network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TcknEevWThkc"
   },
   "source": [
    "## **References**\n",
    "\n",
    "- https://towardsdatascience.com/transfer-learning-using-mobilenet-and-keras-c75daf7ff299\n",
    "- https://www.alibabacloud.com/blog/part-3-image-classification-using-features-extracted-by-transfer-learning-in-keras_595291\n",
    "- https://www.tensorflow.org/tutorials/images/transfer_learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1wB5Aokma-iT"
   },
   "source": [
    "# **END OF NOTEBOOK**\n",
    "\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gZOdscaqa-X4"
   },
   "source": [
    "## **Extension**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yx6vjpOZhY7I"
   },
   "source": [
    "**Download data**\n",
    "\n",
    "We can download  images very easy from Google Image Search. To save sometime we have done this for you. But there is the code to download images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0VPj2ftPOiTF"
   },
   "outputs": [],
   "source": [
    "## An example to download images for crows from google_image package \n",
    "\n",
    "# !pip install google_images_download\n",
    "# from google_images_download import google_images_download\n",
    "# response = google_images_download.googleimagesdownload()\n",
    "# arguments = {\"keywords\":\"crow\",\"limit\":100,\"print_urls\":False, \"format\":\"jpg\", \"size\":\">400*300\"}\n",
    "# paths = response.download(arguments)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Transfer Learning Workshops_1",
   "provenance": [
    {
     "file_id": "1Zj_uP3cHnpTX1-OlBRb_Jral4e8hyvMH",
     "timestamp": 1579491039253
    },
    {
     "file_id": "1CW6pLPp20MjAG-sv1KZR8d889ma0R4Q7",
     "timestamp": 1579459559640
    },
    {
     "file_id": "1cDjnKMghnpSIHFARdh6cA0wEtwWgUET-",
     "timestamp": 1579442631977
    },
    {
     "file_id": "1yG_k5baE9WLxrKcfvAc-6plS-Hf7iOtT",
     "timestamp": 1578689550903
    }
   ]
  },
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
