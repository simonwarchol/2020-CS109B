{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109B Data Science 2: Advanced Topics in Data Science \n",
    "\n",
    "\n",
    "## Advanced Section: Homework 8: Reinforcement Learning [50 pts]\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Spring 2020**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Mark Glickman and Chris Tanner<br/>\n",
    "\n",
    "**DISCLAIMER**: No public reproduction of this homework nor its solution is allowed without the explicit consent of their authors.\n",
    "\n",
    "\n",
    "\n",
    "<hr style=\"height:2pt\">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "#PLEASE RUN THIS CELL \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### INSTRUCTIONS\n",
    "\n",
    "- To submit your assignment follow the instructions given in Canvas.\n",
    "- Restart the kernel and run the whole notebook again before you submit.\n",
    "- Do not submit a notebook that is excessively long because output was not suppressed or otherwise limited. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Numpy and plotting libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class='exercise'><b> Question 1: Basic RL algorithms [20 points]</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "You will explore the behavior of SARSA and Q-learning in the cliffworld domain\n",
    "(similar to the example given on p132 of Sutton and Barto).\n",
    "\n",
    "Agents begin in the start state and can take actions in the set {up, down, left, right}. The episode ends when they reach the goal state, where they receive a reward of 50. Moving or bumping into a wall incurs a small -1 penalty, while falling off the cliff incurs a greviously painful -50 penalty.\n",
    "\n",
    "\n",
    "**1.1**  [4 pts] Complete function `epsilon_greedy_action` that while being in `state`, chooses a random action with probability $\\epsilon$ , or chooses a best action from Q with probability $1-\\epsilon$.\n",
    "\n",
    "**1.2**  [4 pts]  Implement the update steps of SARSA and Q-Learning (p130-131 of Sutton and Barto) in function `do_simulations`. Assume SARSA approximates the Q-function following an $\\epsilon$-greedy action strategy.\n",
    "\n",
    "**1.3**  [4 pts] Run both of these algorithms, let the step size $\\alpha$ be 0.1, and initialize Q (i.e. pass Q initial) to three possible settings:\n",
    "* Q initial = -50\n",
    "* Q initial = 0\n",
    "* Q initial = 50\n",
    "Let the discount factor $\\gamma = 0.95$  and assume an $\\epsilon$-greedy action strategy with $\\epsilon$ = 0.1. Keep the environment deterministic. \n",
    "\n",
    "For each pair of algorithm and parameter setting, run 50 separate trials. Let each trial run for at least 5000 iterations of experience and at least 100 episodes (that is, the number of iterations will be max(5000, iterations it takes to run 100 episodes)).\n",
    "\n",
    "REMARK: you will use the learned policies in the next questions. We encourage you to save the results of the simulations on a dictionary you can easily access by `(method, Q_initial)` for later reuse.\n",
    "\n",
    "**1.4**  [4 pts] Plot the learned policies returned by `run_simulations` on the different scenarios described in 2.3 (6 in total: 2 methods x 3 Q initial values). Use function `plot_policy` we provide to make the figures.\n",
    "\n",
    "**1.5**  [4 pts] Plot the policy that solves the MDP, i.e., `true_Q`. Compare and explain the resulting plots from 2.4 plots and the `true_Q` plot.\n",
    "\n",
    "**1.6** \\[OPTIONAL\\] Plot the average cumulative reward (no discount factor, averaged across the 50 trials) vs. the number of episodes for each case, for the first 100 episodes. Use a different color for each algorithm, and a different line style for the different parameter settings. You may use `episode_rewards` results returned by `run_simulations` from each of the simulations.\n",
    " \n",
    "**1.7** \\[OPTIONAL\\] Using the values of Q(s, a) you obtained for each algorithm, parameter setting, and trial, let your agent run for 5 additional episodes, but with different random action probabilities $\\epsilon$ in {0, 0.01, 0.1, 0.25, 0.33, 0.5}. Store the cumulative reward obtained in each case. This metric captures how well the agent is doing once it's 'done' learning (that is, is it getting close to an optimal reward?) at different levels of action stochasticity. Then make a similar plot as in 2.6 (different color for each algorithm, different line-type for each parameter setting), but with $\\epsilon$ on the x-axis and post-training, 5-episode cumulative reward (averaged over the 50 trials) on the y-axis. You may use `evaluate_static_policy` to compute the cumulative reward evaluated over 5 additional episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Create cliffworld\n",
    "\n",
    "Run the following two cells to create a cliffworld. You will learn policies by running SARSA and Q-learning on it! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "from gridworld import GridWorld\n",
    "MAX_STEPS_PER_EPISODE = 100 # Prevent infinitely long episodes\n",
    "\n",
    "\n",
    "def cliffworld():\n",
    "    \"\"\"Construct the \"cliffworld\" environment.\"\"\"\n",
    "    return GridWorld(\n",
    "        maze=[\n",
    "            '#######',\n",
    "            '#.....#',\n",
    "            '#.##..#',\n",
    "            '#o...*#',\n",
    "            '#XXXXX#',\n",
    "            '#######'\n",
    "        ],\n",
    "        rewards={\n",
    "            '*': 50,         # gain 50 points for reaching goal\n",
    "            'X': -50,        # lose 50 points for falling down\n",
    "            'moved': -1,     # lose 1 point for walking\n",
    "            'hit-wall': -1   # lose 1 point for stumbling into a wall\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Create cliffworld\n",
    "env = cliffworld()\n",
    "\n",
    "# Paint cliffworld\n",
    "def square(x,y,**kw):\n",
    "  plt.axhspan(y,y+1,xmin=x/7,xmax=(x+1)/7,**kw)\n",
    "\n",
    "colors = {'#': 'black', '.': 'burlywood', 'X': 'red', '*': 'green', 'o': 'yellow'}\n",
    "\n",
    "plt.figure(figsize=(7,6))\n",
    "\n",
    "for y,row in enumerate(env.maze.topology):\n",
    "  for x,cell in enumerate(row):\n",
    "    square(x,5-y,color=colors[cell])\n",
    "    \n",
    "plt.axis('off')\n",
    "plt.text(1.5,2.5,'Startüòê', va='center', ha='center', fontweight='bold', fontsize=14)\n",
    "plt.text(5.5,2.5,\"GoalüòÄ\\n(+50)\", va='center', ha='center', fontweight='bold', fontsize=14)\n",
    "plt.text(3.5,1.5,\"‚ò†Ô∏è Cliff ‚ò†Ô∏è\\n(-50 for falling)\", va='center', ha='center', fontweight='bold', fontsize=14)\n",
    "plt.text(3.5,5.5,\"Walls\\n(Block motion, -1 for hitting)\", color='white', va='center', ha='center', fontweight='bold', fontsize=14)\n",
    "plt.text(3.5,4.5,\"Floor\\n(Allows motion, -1 for walking)\", va='center', ha='center', fontweight='bold', fontsize=14)\n",
    "plt.xlim(0,7)\n",
    "plt.ylim(0,6)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "1.1",
    "deletable": false,
    "editable": false
   },
   "source": [
    "**1.1**  [4 pts] Complete function `epsilon_greedy_action` that while being in `state`, chooses a random action with probability $\\epsilon$ , or chooses a best action from Q with probability $1-\\epsilon$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# 1.1\n",
    "\n",
    "def epsilon_greedy_action(state, Q, epsilon=0.1):\n",
    "    \"\"\"Select a random action with probability epsilon or the action suggested\n",
    "    by Q with probability 1-epsilon.\n",
    "    Inputs:\n",
    "    -state: current state.\n",
    "    -Q: 2D numpy array of dimensions (num_states, num_actions).\n",
    "    -epsilon: probability of randomizing an action.\n",
    "    \n",
    "    Retuns: action.\"\"\"\n",
    "    # your code here\n",
    "\n",
    "    # end your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "1.2",
    "deletable": false,
    "editable": false
   },
   "source": [
    "**1.2**  [4 pts]  Implement the update steps of SARSA and Q-Learning (p130-131 of Sutton and Barto) in function `do_simulations`. Assume SARSA approximates the Q-function following an $\\epsilon$-greedy action strategy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# 1.2\n",
    "\n",
    "def run_simulation(\n",
    "        # Common parameters\n",
    "        env,\n",
    "        method,\n",
    "        min_num_episodes=100,\n",
    "        min_num_iters=5000,\n",
    "        epsilon=0.1,\n",
    "        discount=0.95,\n",
    "        # SARSA/Q-learning parameters\n",
    "        step_size=0.5,\n",
    "        Q_initial=0.0,\n",
    "    ):\n",
    "    # Ensure valid parameters\n",
    "    if method not in ('SARSA', 'Q-learning'):\n",
    "        raise ValueError(\"method not in {SARSA, Q-learning}\")\n",
    "\n",
    "    # Initialize arrays for our estimate of Q and observations about T and R,\n",
    "    # and our list of rewards by episode\n",
    "    num_states, num_actions = env.num_states, env.num_actions\n",
    "    Q = np.zeros((num_states, num_actions)) + Q_initial\n",
    "    observed_T_counts = np.zeros((num_states, num_actions, num_states))\n",
    "    observed_R_values = np.zeros((num_states, num_actions, num_states))\n",
    "    episode_rewards = []\n",
    "    num_cliff_falls = 0\n",
    "    global_iter = 0\n",
    "\n",
    "    # Loop through episodes\n",
    "    while len(episode_rewards) < min_num_episodes or global_iter < min_num_iters:\n",
    "        # Reset environment and episode-specific counters\n",
    "        env.reset()\n",
    "        episode_step = 0\n",
    "        episode_reward = 0\n",
    "\n",
    "        # Get our starting state\n",
    "        s1 = env.observe()\n",
    "\n",
    "        # Loop until the episode completes\n",
    "        while not env.is_terminal(s1) and episode_step < MAX_STEPS_PER_EPISODE:\n",
    "            # Take eps-best action & receive reward\n",
    "            a = epsilon_greedy_action(s1, Q, epsilon)\n",
    "            s2, r = env.perform_action(a)\n",
    "\n",
    "            # Update counters\n",
    "            episode_step += 1\n",
    "            episode_reward += r\n",
    "            observed_T_counts[s1][a][s2] += 1\n",
    "            observed_R_values[s1][a][s2] = r\n",
    "            num_cliff_falls += env.is_cliff(s2)\n",
    "\n",
    "            # Use one of the RL methods to update Q\n",
    "            if method == 'SARSA':\n",
    "                \"\"\"Implements SARSA update step in Q from Section 6.4, Sutton and\n",
    "                Barto (p130). SARSA chooses next action based on `epsilon_greedy_action`.\"\"\"\n",
    "                pass\n",
    "                # your code here\n",
    "\n",
    "                # end your code here\n",
    "                \n",
    "            elif method == 'Q-learning':\n",
    "                \"\"\"Implement Q-learning update step from Section 6.4, Sutton\n",
    "                and Barto (p131)\"\"\"\n",
    "                pass\n",
    "                # your code here\n",
    "\n",
    "                # end your code here\n",
    "                \n",
    "            s1 = s2\n",
    "            global_iter += 1\n",
    "\n",
    "        episode_rewards.append(episode_reward)\n",
    "\n",
    "    return { 'Q': Q,\n",
    "            'num_cliff_falls': num_cliff_falls,\n",
    "            'episode_rewards': np.array(episode_rewards) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Run SARSA and Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "1.3",
    "deletable": false,
    "editable": false
   },
   "source": [
    "**1.3**  [4 pts] Run both of these algorithms, let the step size $\\alpha$ be 0.1, and initialize Q (i.e. pass Q initial) to three possible settings:\n",
    "* Q initial = -50\n",
    "* Q initial = 0\n",
    "* Q initial = 50\n",
    "Let the discount factor $\\gamma = 0.95$  and assume an $\\epsilon$-greedy action strategy with $\\epsilon$ = 0.1. Keep the environment deterministic.\n",
    "\n",
    "For each pair of algorithm and parameter setting, run 50 separate trials. Let each trial run for at least 5000 iterations of experience and at least 100 episodes (that is, the number of iterations will be max(5000, iterations it takes to run 100 episodes)).\n",
    "\n",
    "REMARK: you will use the learned policies in the next questions. We encourage you to save the results of the simulations on a dictionary you can easily access by `(method, Q_initial)` for later reuse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# 1.3\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Plot Policy function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def plot_policy(env, Q):\n",
    "    row_count, col_count = env.maze_dimensions\n",
    "    maze_dims = (row_count, col_count)\n",
    "    value_function = np.reshape(np.max(Q, 1), maze_dims)\n",
    "    policy_function = np.reshape(np.argmax(Q, 1), maze_dims)\n",
    "    wall_info = .5 + np.zeros(maze_dims)\n",
    "    wall_mask = np.zeros(maze_dims)\n",
    "    for row in range(row_count):\n",
    "        for col in range(col_count):\n",
    "            if env.maze.topology[row][col] == '#':\n",
    "                wall_mask[row,col] = 1\n",
    "    wall_info = np.ma.masked_where(wall_mask==0, wall_info)\n",
    "    value_function *= (1-wall_mask)**2\n",
    "    plt.imshow(value_function, interpolation='none', cmap='jet')\n",
    "    plt.colorbar(label='Value Function')\n",
    "    plt.imshow(wall_info, interpolation='none' , cmap='gray')\n",
    "    y,x = env.maze.start_coords\n",
    "    plt.text(x,y,'start', color='gray', fontsize=14, va='center', ha='center', fontweight='bold')\n",
    "    y,x = env.maze.goal_coords\n",
    "    plt.text(x,y,'goal', color='yellow', fontsize=14, va='center', ha='center', fontweight='bold')\n",
    "    for row in range( row_count ):\n",
    "        for col in range( col_count ):\n",
    "            if wall_mask[row][col] == 1:\n",
    "                continue\n",
    "            if policy_function[row,col] == 0:\n",
    "                dx = 0; dy = -.5\n",
    "            if policy_function[row,col] == 1:\n",
    "                dx = 0; dy = .5\n",
    "            if policy_function[row,col] == 2:\n",
    "                dx = .5; dy = 0\n",
    "            if policy_function[row,col] == 3:\n",
    "                dx = -.5; dy = 0\n",
    "            plt.arrow(col, row, dx, dy,\n",
    "                shape='full', fc='w' , ec='w' , lw=3, length_includes_head=True, head_width=.2)\n",
    "    plt.xlabel(\"X-Coordinate\")\n",
    "    plt.ylabel(\"Y-Coordinate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "1.4",
    "deletable": false,
    "editable": false
   },
   "source": [
    "**1.4**  [4 pts] Plot the learned policies returned by `run_simulations` on the different scenarios described in 2.3 (6 in total: 2 methods x 3 Q initial values). Use function `plot_policy` we provide to make the figures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# 1.4\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### HW8 Commpletion\n",
    "\n",
    "For the next question, you need to use the functions you completed in HW8. Complete the skeleton of these functions before attempting 2.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "class MDP(object):\n",
    "    \"\"\"Wrapper for a discrete Markov decision process that makes shape checks\"\"\"\n",
    "    \n",
    "    def __init__(self, T, R, discount):\n",
    "        \"\"\"Initialize the Markov Decision Process.\n",
    "        - `T` should be a 3D array whose dimensions represent initial states,\n",
    "          actions, and next states, respectively, and whose values represent\n",
    "          transition probabilities.\n",
    "        - `R` should be a 1D array describing rewards for beginning each\n",
    "          timestep in a particular state (or a 3D array like `T`). It will be\n",
    "          transformed into the appropriate 3D shape.\n",
    "        - `discount` should be a value in [0,1) controlling the decay of future\n",
    "          rewards.\"\"\"\n",
    "        Ds, Da, _ = T.shape\n",
    "        if T.shape not in [(Ds, Da, Ds)]:\n",
    "            raise ValueError(\"T should be in R^|S|x|A|x|S|\")\n",
    "        if R.shape not in [(Ds, Da, Ds), (Ds,)]:\n",
    "            raise ValueError(\"R should be in R^|S| or like T\")\n",
    "        if discount < 0 or discount >= 1:\n",
    "            raise ValueError(\"discount should be in [0,1)\")\n",
    "        if R.shape == (Ds,):  # Expand R if necessary\n",
    "            R = np.array([[[R[s1] for s2 in range(Ds)] for a in range(Da)] for s1 in range(Ds)])\n",
    "        self.T = T\n",
    "        self.R = R\n",
    "        self.discount = discount\n",
    "        self.num_states = Ds\n",
    "        self.num_actions = Da\n",
    "        self.states = np.arange(Ds)\n",
    "        self.actions = np.arange(Da)\n",
    "      \n",
    "    \n",
    "def iterative_value_estimation(mdp, policy, tol=1e-5):\n",
    "    \"\"\"Value estimation algorithm from page 75, Sutton and Barto. Returns an\n",
    "    estimate of the value of a given policy under the MDP (with the number of\n",
    "    iterations required to reach specified tolerance).\"\"\"\n",
    "    V = np.zeros(mdp.num_states)\n",
    "    num_iters = 0\n",
    "    \n",
    "    # your code here\n",
    "    \n",
    "    \n",
    "    # your code ends here\n",
    "    assert Q.shape == (mdp.num_states, mdp.num_actions)\n",
    "    return Q\n",
    "\n",
    "\n",
    "def Q_function(mdp, policy, tol=1e-5):\n",
    "    \"\"\"Q function from Equation 4.6, Sutton and Barto. For each state and\n",
    "    action, returns the value of performing the action at that state, then\n",
    "    following the policy thereafter.\"\"\"\n",
    "    # your code here\n",
    "\n",
    "    \n",
    "    # your code ends here\n",
    "    assert Q.shape == (mdp.num_states, mdp.num_actions)\n",
    "    return Q\n",
    "    \n",
    "\n",
    "def policy_iteration(mdp, init_policy=None, tol=1e-5):\n",
    "    \"\"\"Policy iteration algorithm from page 80, Sutton and Barto.\n",
    "    Iteratively transform the initial policy to become optimal.\n",
    "    Return the full path.\"\"\"\n",
    "    if init_policy is None:\n",
    "        init_policy = np.zeros(mdp.num_states, dtype=int)\n",
    "    policies = [np.array(init_policy)]\n",
    "    \n",
    "    # your code here\n",
    "\n",
    "    \n",
    "    # end your code here\n",
    "    return policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "1.5",
    "deletable": false,
    "editable": false
   },
   "source": [
    "**1.5**  [4 pts] Plot the policy that solves the MDP, i.e., `true_Q`. Compare and explain the resulting plots from 2.4 plots and the `true_Q` plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def true_Q_function(env, discount=0.95):\n",
    "    \"\"\"Return the true value of the Q function using the actual MDP / Q1 code.\n",
    "\n",
    "    NOTE: Please only use this for testing/comparison, not policy learning!\"\"\"\n",
    "    true_T, true_R = env.as_mdp()\n",
    "    true_mdp = MDP(true_T, true_R, discount)\n",
    "    true_pi_star = policy_iteration(true_mdp)[-1]\n",
    "    return Q_function(true_mdp, true_pi_star)\n",
    "\n",
    "true_Q = true_Q_function(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# 2.5\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "**Explain your results:**\n",
    "\n",
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "1.6",
    "deletable": false,
    "editable": false
   },
   "source": [
    "**1.6** \\[OPTIONAL\\] Plot the average cumulative reward (no discount factor, averaged across the 50 trials) vs. the number of episodes for each case, for the first 100 episodes. Use a different color for each algorithm, and a different line style for the different parameter settings. You may use `episode_rewards` results returned by `run_simulations` from each of the simulations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# 2.6\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "**Explain your results:**\n",
    "\n",
    "*your explanation here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "1.7",
    "deletable": false,
    "editable": false
   },
   "source": [
    "**1.7** \\[OPTIONAL\\] Using the values of Q(s, a) you obtained for each algorithm, parameter setting, and trial, let your agent run for 5 additional episodes, but with different random action probabilities $\\epsilon$ in {0, 0.01, 0.1, 0.25, 0.33, 0.5}. Store the cumulative reward obtained in each case. This metric captures how well the agent is doing once it's 'done' learning (that is, is it getting close to an optimal reward?) at different levels of action stochasticity. Then make a similar plot as in 2.6 (different color for each algorithm, different line-type for each parameter setting), but with $\\epsilon$ on the x-axis and post-training, 5-episode cumulative reward (averaged over the 50 trials) on the y-axis. You may use `evaluate_static_policy` to compute the cumulative reward evaluated over 5 additional episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def evaluate_static_policy(env, Q, num_episodes=100, epsilon=0):\n",
    "    \"\"\"Returns cumulative reward following a given Q-function\n",
    "    without updating Q during `num_episodes` or time steps.\"\"\"\n",
    "    episode_rewards = []\n",
    "    while len(episode_rewards) < num_episodes:\n",
    "        episode_reward = 0\n",
    "        episode_iter = 0\n",
    "        env.reset()\n",
    "        s1 = env.observe()\n",
    "        while not env.is_terminal(s1) and episode_iter < MAX_STEPS_PER_EPISODE:\n",
    "            a = epsilon_greedy_action(s1, Q, epsilon)\n",
    "            s1, r = env.perform_action(a)\n",
    "            episode_reward += r\n",
    "            episode_iter += 1\n",
    "        episode_rewards.append(episode_reward)\n",
    "    return np.sum(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# 2.7\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "**Explain your results:**\n",
    "\n",
    "*your explanation here*\n"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
