{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109B Data Science 2: Advanced Topics in Data Science \n",
    "\n",
    "##  Homework 8: Reinforcement Learning [100 pts]\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Spring 2020**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Mark Glickman and Chris Tanner<br/>\n",
    "\n",
    "**DISCLAIMER**: No public reproduction of this homework nor its solution is allowed without the explicit consent of their authors.\n",
    "\n",
    "\n",
    "\n",
    "<hr style=\"height:2pt\">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "#PLEASE RUN THIS CELL \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### INSTRUCTIONS\n",
    "\n",
    "- To submit your assignment follow the instructions given in Canvas.\n",
    "- Restart the kernel and run the whole notebook again before you submit.\n",
    "- Do not submit a notebook that is excessively long because output was not suppressed or otherwise limited. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Numpy and plotting libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The objective of this homework assignment is to get a taste of implementing a planning algorithm in a very simple setting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class='exercise'><b>  Markov Decision Process [100 points] </b></div>\n",
    "\n",
    "\n",
    "We have a hallway consisting of 5 blocks (states 0-4). There are two actions, which deterministically move the agent to the left or the right. More explicitly: Performing action “left” in state 0 keeps you in state 0, moves you from state 1 to state 0, from state 2 to state 1, state 3 to state 2, and state 4 to state 3. Performing action “right” in state 4 keeps you in state 4, moves you from state 3 to state 4, from state 2 to state 3, from state 1 to state 2, and from state 0 to state 1. The agent receives a reward of -1.0 if it starts any iteration in state 0, state 1, state 2, or state 3. The agent receives a reward of +10.0 if it starts in state 4. Let the discount factor γ = 0.75.\n",
    "\n",
    "We provide class MDP that instantiates an object representing a Markov decision process and verifies shapes.\n",
    "\n",
    "**1.1** MDP proble [10 pts]: Build an MDP representing the hallway setting described above, by completing the function `build_hallway_mdp()`. You need to specify the array T that encodes the transitions from state and actions into next states; and a reward vector R that specifies the reward for being at a certain state.\n",
    "\n",
    "**1.2**  Policy Evaluation [20 pts]: Initialize a policy “left” for every state (a 1D numpy array). Implement policy evaluation as described in lecture (also in Chapter 4 of [Sutton and Barto](http://incompleteideas.net/book/RLbook2018.pdf)). That is, for each possible starting state, what is the expected sum of future rewards for this policy? Using an iterative approach, how many iterations did it take for the value of the policy to converge to a precision of 10−5? \n",
    "\n",
    "**1.3**  Q-function Computation [20 pts]: Compute the Q-function for the `always_left` policy above. Do you see any opportunties for policy improvement?\n",
    "\n",
    "**1.4** Policy Iteration [20 pts]: Using your solutions to questions 1.2 and 1.3 above, implement policy iteration. Report the sequence of policies you find starting with the policy “left” in every state. How many rounds of policy iteration are required to converge to the optimal policy? \n",
    "\n",
    "**1.5** [10 pts] What are the effects of different choices of the discount factor on the convergence of policy evaluation? Run policy evaluation for discount factor $\\gamma \\in [ 10^{-12}, 10^{-3}, 0.1, 0.33, 0.67, 0.9, 0.95, 0.99]$.\n",
    "\n",
    "**1.6** [20 pts] What happens if the transitions are stochastic? Recode the MDP with probability of switching to the opposite action of 0.1. What are now the values when following the optimal policy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "class MDP(object):\n",
    "    \"\"\"Wrapper for a discrete Markov decision process that makes shape checks\"\"\"\n",
    "    \n",
    "    def __init__(self, T, R, discount):\n",
    "        \"\"\"Initialize the Markov Decision Process.\n",
    "        - `T` should be a 3D array whose dimensions represent initial states,\n",
    "          actions, and next states, respectively, and whose values represent\n",
    "          transition probabilities.\n",
    "        - `R` should be a 1D array describing rewards for beginning each\n",
    "          timestep in a particular state (or a 3D array like `T`). It will be\n",
    "          transformed into the appropriate 3D shape.\n",
    "        - `discount` should be a value in [0,1) controlling the decay of future\n",
    "          rewards.\"\"\"\n",
    "        Ds, Da, _ = T.shape\n",
    "        if T.shape not in [(Ds, Da, Ds)]:\n",
    "            raise ValueError(\"T should be in R^|S|x|A|x|S|\")\n",
    "        if R.shape not in [(Ds, Da, Ds), (Ds,)]:\n",
    "            raise ValueError(\"R should be in R^|S| or like T\")\n",
    "        if discount < 0 or discount >= 1:\n",
    "            raise ValueError(\"discount should be in [0,1)\")\n",
    "        if R.shape == (Ds,):  # Expand R if necessary\n",
    "            R = np.array([[[R[s1] for s2 in range(Ds)] for a in range(Da)] for s1 in range(Ds)])\n",
    "        self.T = T\n",
    "        self.R = R\n",
    "        self.discount = discount\n",
    "        self.num_states = Ds\n",
    "        self.num_actions = Da\n",
    "        self.states = np.arange(Ds)\n",
    "        self.actions = np.arange(Da)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "1.1",
    "deletable": false,
    "editable": false
   },
   "source": [
    "**1.1** MDP proble [10 pts]: Build an MDP representing the hallway setting described above, by completing the function `build_hallway_mdp()`. You need to specify the array T that encodes the transitions from state and actions into next states; and a reward vector R that specifies the reward for being at a certain state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def build_hallway_mdp():\n",
    "    \"\"\"Build an MDP representing the hallway setting described.\"\"\"\n",
    "    # your code here\n",
    "    # end of your code here\n",
    "    return MDP(T, R, 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Run for sanity check\n",
    "mdp = build_hallway_mdp()\n",
    "\n",
    "plt.figure(figsize=(5,2))\n",
    "plt.subplot(121, title='Left transitions')\n",
    "plt.imshow(mdp.T[:,0,:])\n",
    "plt.ylabel(\"Initial state\"); plt.xlabel('Next state')\n",
    "plt.subplot(122, title='Right transitions')\n",
    "plt.imshow(mdp.T[:,1,:])\n",
    "plt.ylabel(\"Initial state\"); plt.xlabel('Next state')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "1.2",
    "deletable": false,
    "editable": false
   },
   "source": [
    "**1.2**  Policy Evaluation [20 pts]: Initialize a policy “left” for every state (a 1D numpy array). Implement policy evaluation as described in lecture (also in Chapter 4 of [Sutton and Barto](http://incompleteideas.net/book/RLbook2018.pdf)). That is, for each possible starting state, what is the expected sum of future rewards for this policy? Using an iterative approach, how many iterations did it take for the value of the policy to converge to a precision of 10−5?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def build_always_left_policy():\n",
    "    \"\"\"Build a policy representing the action \"left\" in every state.\"\"\"\n",
    "    # your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def iterative_value_estimation(mdp, policy, tol=1e-5):\n",
    "    \"\"\"Value estimation algorithm from page 75, Sutton and Barto. Returns an\n",
    "    estimate of the value of a given policy under the MDP (with the number of\n",
    "    iterations required to reach specified tolerance).\"\"\"\n",
    "    V = np.zeros(mdp.num_states)\n",
    "    num_iters = 0\n",
    "    \n",
    "    # your code here\n",
    "    # end of your code here\n",
    "    return V, num_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Run for sanity check\n",
    "always_left = build_always_left_policy()\n",
    "\n",
    "values, iters = iterative_value_estimation(mdp, always_left)\n",
    "print('Policy value was:')\n",
    "print(values.round(4))\n",
    "\n",
    "tols = np.logspace(0,-8,9)\n",
    "iters = [iterative_value_estimation(mdp, always_left, tol=tol)[1] for tol in tols]\n",
    "plt.plot(tols, iters, marker='o')\n",
    "plt.xscale('log')\n",
    "plt.xlabel(\"Tolerance\")\n",
    "plt.ylabel(\"Iterations to converge to within tolerance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "1.3",
    "deletable": false,
    "editable": false
   },
   "source": [
    "**1.3**  Q-function Computation [20 pts]: Compute the Q-function for the `always_left` policy above. Do you see any opportunties for policy improvement?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# 1.3\n",
    "def Q_function(mdp, policy, tol=1e-5):\n",
    "    \"\"\"Q function from Equation 4.6, Sutton and Barto. For each state and\n",
    "    action, returns the value of performing the action at that state, then\n",
    "    following the policy thereafter.\"\"\"\n",
    "    # your code here\n",
    "    # end of your code here\n",
    "    assert Q.shape == (mdp.num_states, mdp.num_actions)\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Run for sanity check\n",
    "\n",
    "Q = Q_function(mdp, always_left)\n",
    "print('Q function was:')\n",
    "print(Q.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "1.4",
    "deletable": false,
    "editable": false
   },
   "source": [
    "**1.4** Policy Iteration [20 pts]: Using your solutions to questions 1.2 and 1.3 above, implement policy iteration. Report the sequence of policies you find starting with the policy “left” in every state. How many rounds of policy iteration are required to converge to the optimal policy?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# 1.4\n",
    "def policy_iteration(mdp, init_policy=None, tol=1e-5):\n",
    "    \"\"\"Policy iteration algorithm from page 80, Sutton and Barto.\n",
    "    Iteratively transform the initial policy to become optimal.\n",
    "    Return the full path.\"\"\"\n",
    "    # your code here\n",
    "    # end of your code here\n",
    "    return policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "\n",
    "policy_iters = policy_iteration(mdp, always_left)\n",
    "policy_iters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "1.5",
    "deletable": false,
    "editable": false
   },
   "source": [
    "**1.5** [10 pts] What are the effects of different choices of the discount factor on the convergence of policy evaluation? Run policy evaluation for discount factor $\\gamma \\in [ 10^{-12}, 10^{-3}, 0.1, 0.33, 0.67, 0.9, 0.95, 0.99]$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# 1.5\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "plt.plot(discount_factors, iters_by_factor, marker='o')\n",
    "plt.xlabel('Discount factor $\\gamma$')\n",
    "plt.ylabel('Iterations for value estimate to converge')\n",
    "plt.title(\"Convergence of value estimate by $\\gamma$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "1.6",
    "deletable": false,
    "editable": false
   },
   "source": [
    "**1.6** [20 pts] What happens if the transitions are stochastic? Recode the MDP with probability of switching to the opposite action of 0.1. What are now the values when following the optimal policy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# 1.6\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "*Your answer here*"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
