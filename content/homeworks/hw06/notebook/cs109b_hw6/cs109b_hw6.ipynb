{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109B Data Science 2: Advanced Topics in Data Science \n",
    "## Homework 6 - RNNs\n",
    "\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2020**<br/>\n",
    "**Instructors**: Mark Glickman, Pavlos Protopapas, & Chris Tanner\n",
    "\n",
    "\n",
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "span.sub-q {\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A; \t \n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RUN THIS CELL \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTRUCTIONS\n",
    "\n",
    "<span style=\"color:red\">**Model training can be very slow; start doing this HW early**</span>\n",
    "\n",
    "- To submit your assignment follow the instructions given in Canvas.\n",
    "\n",
    "- This homework can be submitted in pairs.\n",
    "\n",
    "- If you submit individually but you have worked with someone, please include the name of your **one** partner below.\n",
    "- Please restart the kernel and run the entire notebook again before you submit.\n",
    "\n",
    "**Names of person you have worked with goes here:**\n",
    "<br><BR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"theme\"> Overview: Named Entity Recognition Challenge</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named entity recognition (NER) seeks to locate and classify named entities present in unstructured text into predefined categories such as organizations, locations, expressions of times, names of persons, etc. This technique is often used in real use cases such as classifying content for news providers, efficient search algorithms over large corpora and content-based recommendation systems. \n",
    "\n",
    "This represents an interesting \"many-to-many\" problem, allowing us to experiment with recurrent architectures and compare their performances against other models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4-tf\n",
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import backend\n",
    "\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.layers import Input, SimpleRNN, Embedding, Dense, TimeDistributed, GRU, \\\n",
    "                          Dropout, Bidirectional, Conv1D, BatchNormalization\n",
    "\n",
    "print(tf.keras.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for repeatable results\n",
    "np.random.seed(123)\n",
    "tf.random.set_seed(456)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rUkgUGwJXUcH"
   },
   "source": [
    "<div class=\"theme\"> Part 1: Data </div>\n",
    "Read `HW6_data.csv` into a pandas dataframe using the provided code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>London</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>protest</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>war</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>in</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demand</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sentence #           Word  POS    Tag\n",
       "0   Sentence: 1      Thousands  NNS      O\n",
       "1   Sentence: 1             of   IN      O\n",
       "2   Sentence: 1  demonstrators  NNS      O\n",
       "3   Sentence: 1           have  VBP      O\n",
       "4   Sentence: 1        marched  VBN      O\n",
       "5   Sentence: 1        through   IN      O\n",
       "6   Sentence: 1         London  NNP  B-geo\n",
       "7   Sentence: 1             to   TO      O\n",
       "8   Sentence: 1        protest   VB      O\n",
       "9   Sentence: 1            the   DT      O\n",
       "10  Sentence: 1            war   NN      O\n",
       "11  Sentence: 1             in   IN      O\n",
       "12  Sentence: 1           Iraq  NNP  B-geo\n",
       "13  Sentence: 1            and   CC      O\n",
       "14  Sentence: 1         demand   VB      O"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Given code\n",
    "path_dataset = './data/HW6_data.csv'\n",
    "data = pd.read_csv(path_dataset,\n",
    "                   encoding=\"latin1\")\n",
    "data = data.fillna(method=\"ffill\")\n",
    "data.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have a dataset with sentences (```Sentence #``` column), each composed of words (```Word``` column) with part-of-speech tagging (```POS``` tagging) and inside–outside–beginning (IOB) named entity tags (```Tag``` column) attached. ```POS``` will not be used for this homework. We will predict ```Tag``` using only the words themselves.\n",
    "\n",
    "Essential info about entities:\n",
    "* geo = Geographical Entity\n",
    "* org = Organization\n",
    "* per = Person\n",
    "* gpe = Geopolitical Entity\n",
    "* tim = Time indicator\n",
    "* art = Artifact\n",
    "* eve = Event\n",
    "* nat = Natural Phenomenon\n",
    "\n",
    "IOB prefix:\n",
    "* B: beginning of named entity\n",
    "* I: inside of named entity\n",
    "* O: outside of named entity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Question 1: Data [20 points total]</b></div>\n",
    "\n",
    "**1.1** Create a list of unique words found in the 'Word' column and sort it in alphabetic order. Then append the special word \"ENDPAD\" to the end of the list, and assign it to the variable ```words```. Store the length of this list as ```n_words```. **Print your results for `n_words`**\n",
    "\n",
    "**1.2** Create a list of unique tags and sort it in alphabetic order. Then append the special word \"PAD\" to the end of the list, and assign it to the variable ```tags```. Store the length of this list as ```n_tags```. **Print your results for `n_tags`**\n",
    "\n",
    "**1.3** Process the data into a list of sentences where each sentence is a list of (word, tag) tuples. Here is an example of how the first sentence in the list should look:\n",
    "\n",
    "[('Thousands', 'O'),\n",
    " ('of',  'O'),\n",
    " ('demonstrators', 'O'),\n",
    " ('have', 'O'),\n",
    " ('marched', 'O'),\n",
    " ('through', 'O'),\n",
    " ('London', 'B-geo'),\n",
    " ('to', 'O'),\n",
    " ('protest', 'O'),\n",
    " ('the', 'O'),\n",
    " ('war', 'O'),\n",
    " ('in', 'O'),\n",
    " ('Iraq', 'B-geo'),\n",
    " ('and', 'O'),\n",
    " ('demand', 'O'),\n",
    " ('the', 'O'),\n",
    " ('withdrawal', 'O'),\n",
    " ('of', 'O'),\n",
    " ('British', 'B-gpe'),\n",
    " ('troops', 'O'),\n",
    " ('from', 'O'),\n",
    " ('that', 'O'),\n",
    " ('country', 'O'),\n",
    " ('.', 'O')]\n",
    " \n",
    "**1.4** Find out the number of words in the longest sentence, and store it to variable ```max_len```. **Print your results for `max_len`.**\n",
    "\n",
    "**1.5** It's now time to convert the sentences data in a suitable format for the RNNs training/evaluation procedures. Create a ```word2idx``` dictionary mapping distinct words from the dataset into distinct integers. Also create a ```idx2word``` dictionary.\n",
    "\n",
    "**1.6** Prepare the predictors matrix ```X```, as a list of lists, where each inner list is a sequence of words mapped into integers accordly to the ```word2idx``` dictionary. \n",
    "\n",
    "**1.7** Apply the keras ```pad_sequences``` function to standardize the predictors. You should retrieve a matrix with all padded sentences and length equal to ```max_len``` previously computed. The dimensionality should therefore be equal to ```[# of sentences, max_len]```. Run the provided cell to print your results. Your ```X[i]``` now should be something similar to this:\n",
    "\n",
    "`[ 8193 27727 31033 33289 22577 33464 23723 16665 33464 31142 31319 28267\n",
    " 27700 33246 28646 16052    21 16915 17349  7924 32879 32985 18238 23555\n",
    "    24 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178\n",
    " 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178\n",
    " 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178\n",
    " 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178\n",
    " 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178\n",
    " 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178\n",
    " 35178 35178 35178 35178 35178 35178 35178 35178]`\n",
    " \n",
    "**1.8** Create a ```tag2idx``` dictionary mapping distinct named entity tags from the dataset into distinct integers. Also create a ```idx2tag``` dictionary.\n",
    "\n",
    "**1.9** Prepare targets matrix ```Y```, as a list of lists,where each inner list is a sequence of tags mapped into integers accordly to the ```tag2idx``` dictionary.\n",
    "\n",
    "**1.10** apply the keras ```pad_sequences``` function to standardize the targets. Inject the ```PAD``` tag for the padding words. You should retrieve a matrix with all padded sentences'tags and length equal to ```max_length``` previously computed. \n",
    "\n",
    "**1.11** Use the ```to_categorical``` keras function to one-hot encode the tags. Now your ```Y``` should have dimension ```[# of sentences, max_len, n_tags]```. Run the provided cell to print your results.\n",
    "\n",
    "**1.12** Split the dataset into train and test sets (test 10%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hBtmANNuuS6h"
   },
   "source": [
    "## Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1** Create a list of unique words found in the 'Word' column and sort it in alphabetic order. Then append the special word \"ENDPAD\" to the end of the list, and assign it to the variable ```words```. Store the length of this list as ```n_words```. **Print your results for `n_words`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "words = data.sort_values('Word')['Word'].unique().tolist()\n",
    "words.append('ENDPAD')\n",
    "n_words = len(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35179\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to show your results for n_words\n",
    "print(n_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2** Create a list of unique tags and sort it in alphabetic order. Then append the special word \"PAD\" to the end of the list, and assign it to the variable ```tags```. Store the length of this list as ```n_tags```. **Print your results for `n_tags`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "tags = data.sort_values('Tag')['Tag'].unique().tolist()\n",
    "tags.append('PAD')\n",
    "n_tags = len(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "(1048575, 4)\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to show your results for n_tags\n",
    "print(n_tags)\n",
    "print(np.shape(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3** Process the data into a list of sentences where each sentence is a list of (word, tag) tuples. Here is an example of how the first sentence in the list should look:\n",
    "\n",
    "[('Thousands', 'O'),\n",
    " ('of',  'O'),\n",
    " ('demonstrators', 'O'),\n",
    " ('have', 'O'),\n",
    " ('marched', 'O'),\n",
    " ('through', 'O'),\n",
    " ('London', 'B-geo'),\n",
    " ('to', 'O'),\n",
    " ('protest', 'O'),\n",
    " ('the', 'O'),\n",
    " ('war', 'O'),\n",
    " ('in', 'O'),\n",
    " ('Iraq', 'B-geo'),\n",
    " ('and', 'O'),\n",
    " ('demand', 'O'),\n",
    " ('the', 'O'),\n",
    " ('withdrawal', 'O'),\n",
    " ('of', 'O'),\n",
    " ('British', 'B-gpe'),\n",
    " ('troops', 'O'),\n",
    " ('from', 'O'),\n",
    " ('that', 'O'),\n",
    " ('country', 'O'),\n",
    " ('.', 'O')]\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "sentence_list = None\n",
    "# with open(\"sentence_list.pickle\", \"wb\") as fp:   #Pickling\n",
    "#     pickle.dump(sentence_list, fp)\n",
    "try:    \n",
    "    with open(\"sentence_list.pickle\", \"rb\") as fp:   # Unpickling\n",
    "        sentence_list = pickle.load(fp)\n",
    "except:\n",
    "    print(\"No Sentence List File Found\")\n",
    "if sentence_list == None:\n",
    "    sentences = data['Sentence #'].unique().tolist()\n",
    "    sentence_list = [[] for x in range(len(sentences))]\n",
    "    for index, row in data.iterrows():\n",
    "        sentence_number = sentences.index(row['Sentence #'])\n",
    "        sentence_list[sentence_number].append((row['Word'], row['Tag']))\n",
    "    with open(\"sentence_list.pickle\", \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(sentence_list, fp)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Thousands', 'O'),\n",
       " ('of', 'O'),\n",
       " ('demonstrators', 'O'),\n",
       " ('have', 'O'),\n",
       " ('marched', 'O'),\n",
       " ('through', 'O'),\n",
       " ('London', 'B-geo'),\n",
       " ('to', 'O'),\n",
       " ('protest', 'O'),\n",
       " ('the', 'O'),\n",
       " ('war', 'O'),\n",
       " ('in', 'O'),\n",
       " ('Iraq', 'B-geo'),\n",
       " ('and', 'O'),\n",
       " ('demand', 'O'),\n",
       " ('the', 'O'),\n",
       " ('withdrawal', 'O'),\n",
       " ('of', 'O'),\n",
       " ('British', 'B-gpe'),\n",
       " ('troops', 'O'),\n",
       " ('from', 'O'),\n",
       " ('that', 'O'),\n",
       " ('country', 'O'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4** Find out the number of words in the longest sentence, and store it to variable ```max_len```. **Print your results for `max_len`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for sentence in sentence_list:\n",
    "    if len(sentence) > max_len:\n",
    "        max_len = len(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run this cell to show your results for max_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n"
     ]
    }
   ],
   "source": [
    "print(max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.5** It's now time to convert the sentences data in a suitable format for the RNNs training/evaluation procedures. Create a ```word2idx``` dictionary mapping distinct words from the dataset into distinct integers. Also create a ```idx2word``` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 0,\n",
       " '\"': 1,\n",
       " '#': 2,\n",
       " '#NAME?': 3,\n",
       " '$': 4,\n",
       " '%': 5,\n",
       " '%-plus': 6,\n",
       " '&': 7,\n",
       " \"'\": 8,\n",
       " \"'70s\": 9,\n",
       " \"'80s\": 10,\n",
       " \"'T\": 11,\n",
       " \"'d\": 12,\n",
       " \"'ll\": 13,\n",
       " \"'m\": 14,\n",
       " \"'n\": 15,\n",
       " \"'re\": 16,\n",
       " \"'s\": 17,\n",
       " \"'ve\": 18,\n",
       " '(': 19,\n",
       " ')': 20,\n",
       " ',': 21,\n",
       " '-': 22,\n",
       " '--': 23,\n",
       " '.': 24,\n",
       " '..': 25,\n",
       " '...': 26,\n",
       " '.tv': 27,\n",
       " '/': 28,\n",
       " '0': 29,\n",
       " '0-3': 30,\n",
       " '0-6': 31,\n",
       " '0.01': 32,\n",
       " '0.02': 33,\n",
       " '0.04': 34,\n",
       " '0.050474537': 35,\n",
       " '0.051655093': 36,\n",
       " '0.060092593': 37,\n",
       " '0.068171296': 38,\n",
       " '0.068217593': 39,\n",
       " '0.068263889': 40,\n",
       " '0.068275463': 41,\n",
       " '0.068472222': 42,\n",
       " '0.08': 43,\n",
       " '0.1': 44,\n",
       " '0.110474537': 45,\n",
       " '0.110625': 46,\n",
       " '0.12': 47,\n",
       " '0.161': 48,\n",
       " '0.17': 49,\n",
       " '0.19': 50,\n",
       " '0.193': 51,\n",
       " '0.2': 52,\n",
       " '0.3': 53,\n",
       " '0.35': 54,\n",
       " '0.4': 55,\n",
       " '0.5': 56,\n",
       " '0.55': 57,\n",
       " '0.6': 58,\n",
       " '0.65': 59,\n",
       " '0.7': 60,\n",
       " '0.8': 61,\n",
       " '0.82': 62,\n",
       " '0.9': 63,\n",
       " '01-Feb': 64,\n",
       " '01-Jan': 65,\n",
       " '01-Jun': 66,\n",
       " '02-Feb': 67,\n",
       " '02-Jan': 68,\n",
       " '02-Jun': 69,\n",
       " '02-May': 70,\n",
       " '03-Apr': 71,\n",
       " '03-Feb': 72,\n",
       " '03-Jan': 73,\n",
       " '03-Jun': 74,\n",
       " '03-Mar': 75,\n",
       " '03-May': 76,\n",
       " '04-Feb': 77,\n",
       " '04-Jan': 78,\n",
       " '04-Jun': 79,\n",
       " '05-Apr': 80,\n",
       " '05-Feb': 81,\n",
       " '05-Jan': 82,\n",
       " '05-Jul': 83,\n",
       " '05-Jun': 84,\n",
       " '05-Mar': 85,\n",
       " '06-Apr': 86,\n",
       " '06-Feb': 87,\n",
       " '06-Jan': 88,\n",
       " '06-Jul': 89,\n",
       " '06-Mar': 90,\n",
       " '07-Apr': 91,\n",
       " '07-Aug': 92,\n",
       " '07-Feb': 93,\n",
       " '07-Jun': 94,\n",
       " '07-May': 95,\n",
       " '09-Aug': 96,\n",
       " '09-Feb': 97,\n",
       " '09-Nov': 98,\n",
       " '09-Sep': 99,\n",
       " '1': 100,\n",
       " '1,00,000': 101,\n",
       " '1,000': 102,\n",
       " '1,000-meter': 103,\n",
       " '1,000-year': 104,\n",
       " '1,04,586': 105,\n",
       " '1,050': 106,\n",
       " '1,06,591': 107,\n",
       " '1,07,000': 108,\n",
       " '1,09,000': 109,\n",
       " '1,10,000': 110,\n",
       " '1,100': 111,\n",
       " '1,11,000': 112,\n",
       " '1,12,000': 113,\n",
       " '1,125': 114,\n",
       " '1,15,000': 115,\n",
       " '1,157': 116,\n",
       " '1,17,000': 117,\n",
       " '1,170': 118,\n",
       " '1,20,000': 119,\n",
       " '1,200': 120,\n",
       " '1,247': 121,\n",
       " '1,250': 122,\n",
       " '1,254': 123,\n",
       " '1,26,976': 124,\n",
       " '1,27,000': 125,\n",
       " '1,27,910': 126,\n",
       " '1,275': 127,\n",
       " '1,28,000': 128,\n",
       " '1,30,000': 129,\n",
       " '1,300': 130,\n",
       " '1,31,000': 131,\n",
       " '1,311': 132,\n",
       " '1,32,000': 133,\n",
       " '1,34,503': 134,\n",
       " '1,35,000': 135,\n",
       " '1,38,000': 136,\n",
       " '1,40,000': 137,\n",
       " '1,400': 138,\n",
       " '1,44,000': 139,\n",
       " '1,460': 140,\n",
       " '1,50,000': 141,\n",
       " '1,500': 142,\n",
       " '1,500-meter': 143,\n",
       " '1,500-meters': 144,\n",
       " '1,55,000': 145,\n",
       " '1,59,000': 146,\n",
       " '1,60,000': 147,\n",
       " '1,600': 148,\n",
       " '1,600-kilometer': 149,\n",
       " '1,600-member': 150,\n",
       " '1,630': 151,\n",
       " '1,64,000': 152,\n",
       " '1,675': 153,\n",
       " '1,69,000': 154,\n",
       " '1,70,000': 155,\n",
       " '1,700': 156,\n",
       " '1,700-year-old': 157,\n",
       " '1,72,000': 158,\n",
       " '1,74,000': 159,\n",
       " '1,75,000': 160,\n",
       " '1,76,000': 161,\n",
       " '1,80,000': 162,\n",
       " '1,800': 163,\n",
       " '1,82,000': 164,\n",
       " '1,850': 165,\n",
       " '1,90,000': 166,\n",
       " '1,900': 167,\n",
       " '1,950': 168,\n",
       " '1-0': 169,\n",
       " '1-to-1': 170,\n",
       " '1.06': 171,\n",
       " '1.08': 172,\n",
       " '1.1': 173,\n",
       " '1.10': 174,\n",
       " '1.125': 175,\n",
       " '1.16': 176,\n",
       " '1.17': 177,\n",
       " '1.18': 178,\n",
       " '1.2': 179,\n",
       " '1.224': 180,\n",
       " '1.24': 181,\n",
       " '1.26': 182,\n",
       " '1.27': 183,\n",
       " '1.28.01': 184,\n",
       " '1.28.19': 185,\n",
       " '1.2927': 186,\n",
       " '1.294': 187,\n",
       " '1.3': 188,\n",
       " '1.31': 189,\n",
       " '1.32': 190,\n",
       " '1.329': 191,\n",
       " '1.34': 192,\n",
       " '1.35': 193,\n",
       " '1.36': 194,\n",
       " '1.38.53': 195,\n",
       " '1.4': 196,\n",
       " '1.459': 197,\n",
       " '1.48': 198,\n",
       " '1.5': 199,\n",
       " '1.55': 200,\n",
       " '1.6': 201,\n",
       " '1.61': 202,\n",
       " '1.64': 203,\n",
       " '1.65': 204,\n",
       " '1.66': 205,\n",
       " '1.7': 206,\n",
       " '1.76': 207,\n",
       " '1.8': 208,\n",
       " '1.85': 209,\n",
       " '1.9': 210,\n",
       " '1.92': 211,\n",
       " '10': 212,\n",
       " '10,00,000': 213,\n",
       " '10,000': 214,\n",
       " '10,000-square-kilometer': 215,\n",
       " '10,000-strong': 216,\n",
       " '10-Dec': 217,\n",
       " '10-Jan': 218,\n",
       " '10-day': 219,\n",
       " '10-kilometer': 220,\n",
       " '10-man': 221,\n",
       " '10-member': 222,\n",
       " '10-meter': 223,\n",
       " '10-million': 224,\n",
       " '10-month-old': 225,\n",
       " '10-nation': 226,\n",
       " '10-percent': 227,\n",
       " '10-person': 228,\n",
       " '10-point': 229,\n",
       " '10-story': 230,\n",
       " '10-to-eight': 231,\n",
       " '10-week': 232,\n",
       " '10-year': 233,\n",
       " '10-year-old': 234,\n",
       " '10-years-ago': 235,\n",
       " '10.2': 236,\n",
       " '10.5': 237,\n",
       " '10.7': 238,\n",
       " '10.8': 239,\n",
       " '100': 240,\n",
       " '100-day': 241,\n",
       " '100-meter': 242,\n",
       " '100-meters': 243,\n",
       " '100-million': 244,\n",
       " '100-thousand': 245,\n",
       " '100-year': 246,\n",
       " '100-year-old': 247,\n",
       " '100.8': 248,\n",
       " '1000': 249,\n",
       " '1002': 250,\n",
       " '1004': 251,\n",
       " '100th': 252,\n",
       " '101': 253,\n",
       " '1010': 254,\n",
       " '1014': 255,\n",
       " '101st': 256,\n",
       " '102': 257,\n",
       " '102-member': 258,\n",
       " '102.4': 259,\n",
       " '103': 260,\n",
       " '103.05': 261,\n",
       " '103.9': 262,\n",
       " '1030': 263,\n",
       " '104': 264,\n",
       " '104.1': 265,\n",
       " '104.6': 266,\n",
       " '105': 267,\n",
       " '105.97': 268,\n",
       " '1054': 269,\n",
       " '1055': 270,\n",
       " '106': 271,\n",
       " '106.5': 272,\n",
       " '107': 273,\n",
       " '107.25': 274,\n",
       " '107.41': 275,\n",
       " '1070': 276,\n",
       " '108': 277,\n",
       " '109': 278,\n",
       " '10th': 279,\n",
       " '11': 280,\n",
       " '11,000': 281,\n",
       " '11,300': 282,\n",
       " '11,500': 283,\n",
       " '11,700': 284,\n",
       " '11-day': 285,\n",
       " '11-judge': 286,\n",
       " '11-nation': 287,\n",
       " '11-style': 288,\n",
       " '11-year': 289,\n",
       " '11-year-old': 290,\n",
       " '11.5': 291,\n",
       " '11.6': 292,\n",
       " '11.8': 293,\n",
       " '11.9': 294,\n",
       " '110': 295,\n",
       " '111': 296,\n",
       " '111th': 297,\n",
       " '112': 298,\n",
       " '113': 299,\n",
       " '113.2': 300,\n",
       " '114': 301,\n",
       " '114.8': 302,\n",
       " '115': 303,\n",
       " '115.2': 304,\n",
       " '116': 305,\n",
       " '117': 306,\n",
       " '117-ball': 307,\n",
       " '11705': 308,\n",
       " '11725': 309,\n",
       " '118': 310,\n",
       " '119': 311,\n",
       " '119.48': 312,\n",
       " '11th': 313,\n",
       " '11th-hour': 314,\n",
       " '12': 315,\n",
       " '12,000': 316,\n",
       " '12,012': 317,\n",
       " '12,027': 318,\n",
       " '12,049': 319,\n",
       " '12,348': 320,\n",
       " '12,396': 321,\n",
       " '12,500': 322,\n",
       " '12-country': 323,\n",
       " '12-hour': 324,\n",
       " '12-kilometer-high': 325,\n",
       " '12-member': 326,\n",
       " '12-minute': 327,\n",
       " '12-month': 328,\n",
       " '12-nation': 329,\n",
       " '12-percent': 330,\n",
       " '12-point': 331,\n",
       " '12-story': 332,\n",
       " '12-year': 333,\n",
       " '12-year-civil': 334,\n",
       " '12-year-old': 335,\n",
       " '12.09': 336,\n",
       " '12.3': 337,\n",
       " '12.45': 338,\n",
       " '12.5': 339,\n",
       " '12.5-kilometer': 340,\n",
       " '12.97': 341,\n",
       " '120': 342,\n",
       " '120-member': 343,\n",
       " '120-seat': 344,\n",
       " '1200': 345,\n",
       " '1204': 346,\n",
       " '121.79': 347,\n",
       " '1215': 348,\n",
       " '122': 349,\n",
       " '122.73': 350,\n",
       " '122.8': 351,\n",
       " '123': 352,\n",
       " '123-kilometer': 353,\n",
       " '1236': 354,\n",
       " '124': 355,\n",
       " '125': 356,\n",
       " '126': 357,\n",
       " '126.2': 358,\n",
       " '127': 359,\n",
       " '1278': 360,\n",
       " '128': 361,\n",
       " '128-member': 362,\n",
       " '129': 363,\n",
       " '1291': 364,\n",
       " '1297': 365,\n",
       " '12th': 366,\n",
       " '13': 367,\n",
       " '13,000': 368,\n",
       " '13,090': 369,\n",
       " '13,100': 370,\n",
       " '13,107': 371,\n",
       " '13,406': 372,\n",
       " '13,543': 373,\n",
       " '13,969': 374,\n",
       " '13-member': 375,\n",
       " '13-point': 376,\n",
       " '13-year': 377,\n",
       " '13-year-old': 378,\n",
       " '13.5': 379,\n",
       " '13.625': 380,\n",
       " '13.65': 381,\n",
       " '13.73': 382,\n",
       " '13.857': 383,\n",
       " '130': 384,\n",
       " '130-thousand': 385,\n",
       " '130.6': 386,\n",
       " '13010': 387,\n",
       " '131': 388,\n",
       " '131-5': 389,\n",
       " '132': 390,\n",
       " '132-seat': 391,\n",
       " '133': 392,\n",
       " '1331': 393,\n",
       " '134': 394,\n",
       " '135': 395,\n",
       " '135.6': 396,\n",
       " '136': 397,\n",
       " '137': 398,\n",
       " '138': 399,\n",
       " '138.28': 400,\n",
       " '1386': 401,\n",
       " '139': 402,\n",
       " '1397': 403,\n",
       " '13th': 404,\n",
       " '14': 405,\n",
       " '14,000': 406,\n",
       " '14,706': 407,\n",
       " '14-day': 408,\n",
       " '14-hour': 409,\n",
       " '14-kilometer-long': 410,\n",
       " '14-month': 411,\n",
       " '14-thousand': 412,\n",
       " '14-year': 413,\n",
       " '14-year-old': 414,\n",
       " '14.2': 415,\n",
       " '14.5': 416,\n",
       " '14.7': 417,\n",
       " '14.75': 418,\n",
       " '14.88': 419,\n",
       " '140': 420,\n",
       " '140-kilometers': 421,\n",
       " '141': 422,\n",
       " '141.9': 423,\n",
       " '1419': 424,\n",
       " '142': 425,\n",
       " '143': 426,\n",
       " '144': 427,\n",
       " '145': 428,\n",
       " '145.85': 429,\n",
       " '1452': 430,\n",
       " '146': 431,\n",
       " '147': 432,\n",
       " '148': 433,\n",
       " '148.9': 434,\n",
       " '149': 435,\n",
       " '149.9': 436,\n",
       " '1492': 437,\n",
       " '1493': 438,\n",
       " '1494': 439,\n",
       " '1498': 440,\n",
       " '1499': 441,\n",
       " '14th': 442,\n",
       " '14th-century': 443,\n",
       " '15': 444,\n",
       " '15,000': 445,\n",
       " '15,000-ton': 446,\n",
       " '15-20': 447,\n",
       " '15-Feb': 448,\n",
       " '15-day': 449,\n",
       " '15-kilometer': 450,\n",
       " '15-member': 451,\n",
       " '15-month': 452,\n",
       " '15-nation': 453,\n",
       " '15-second': 454,\n",
       " '15-year': 455,\n",
       " '15-year-old': 456,\n",
       " '15.1': 457,\n",
       " '15.16': 458,\n",
       " '15.5': 459,\n",
       " '15.6': 460,\n",
       " '150': 461,\n",
       " '150-member': 462,\n",
       " '150-meter-long': 463,\n",
       " '150-thousand': 464,\n",
       " '150-year-old': 465,\n",
       " '1500': 466,\n",
       " '1500s': 467,\n",
       " '151': 468,\n",
       " '153.3': 469,\n",
       " '1533': 470,\n",
       " '154': 471,\n",
       " '154th': 472,\n",
       " '155': 473,\n",
       " '1559': 474,\n",
       " '156': 475,\n",
       " '1563': 476,\n",
       " '1569': 477,\n",
       " '157': 478,\n",
       " '1578': 479,\n",
       " '158': 480,\n",
       " '1582': 481,\n",
       " '159': 482,\n",
       " '159th': 483,\n",
       " '15th': 484,\n",
       " '16': 485,\n",
       " '16,000': 486,\n",
       " '16-day': 487,\n",
       " '16-goal': 488,\n",
       " '16-stop': 489,\n",
       " '16-team': 490,\n",
       " '16-year': 491,\n",
       " '16-year-old': 492,\n",
       " '16.125': 493,\n",
       " '16.2': 494,\n",
       " '16.3': 495,\n",
       " '16.7': 496,\n",
       " '16.8': 497,\n",
       " '160': 498,\n",
       " '1600': 499,\n",
       " '1603': 500,\n",
       " '1607': 501,\n",
       " '1609': 502,\n",
       " '161-seat': 503,\n",
       " '1614': 504,\n",
       " '162': 505,\n",
       " '162-7': 506,\n",
       " '162-page': 507,\n",
       " '1621': 508,\n",
       " '1623': 509,\n",
       " '163': 510,\n",
       " '163-seat': 511,\n",
       " '1631': 512,\n",
       " '1632': 513,\n",
       " '1633': 514,\n",
       " '1634': 515,\n",
       " '164': 516,\n",
       " '1644': 517,\n",
       " '1647': 518,\n",
       " '1648': 519,\n",
       " '165': 520,\n",
       " '165-seat': 521,\n",
       " '1650': 522,\n",
       " '1652': 523,\n",
       " '1655': 524,\n",
       " '1657': 525,\n",
       " '166': 526,\n",
       " '1667': 527,\n",
       " '167': 528,\n",
       " '167-seat': 529,\n",
       " '1672': 530,\n",
       " '168': 531,\n",
       " '1682': 532,\n",
       " '169': 533,\n",
       " '169.9': 534,\n",
       " '1697': 535,\n",
       " '16th': 536,\n",
       " '17': 537,\n",
       " '17,000': 538,\n",
       " '17,500': 539,\n",
       " '17,765': 540,\n",
       " '17-Jan': 541,\n",
       " '17-Jun': 542,\n",
       " '17-member': 543,\n",
       " '17-meters': 544,\n",
       " '17-month': 545,\n",
       " '17-year': 546,\n",
       " '17-year-old': 547,\n",
       " '17.09': 548,\n",
       " '17.2': 549,\n",
       " '17.3': 550,\n",
       " '17.6': 551,\n",
       " '17.7': 552,\n",
       " '17.9': 553,\n",
       " '170': 554,\n",
       " '170-meter': 555,\n",
       " '171': 556,\n",
       " '1713': 557,\n",
       " '1715': 558,\n",
       " '1717': 559,\n",
       " '1718': 560,\n",
       " '1719': 561,\n",
       " '1725': 562,\n",
       " '1726': 563,\n",
       " '173': 564,\n",
       " '174': 565,\n",
       " '1745': 566,\n",
       " '1747': 567,\n",
       " '175': 568,\n",
       " '175-run': 569,\n",
       " '1750': 570,\n",
       " '1755': 571,\n",
       " '176': 572,\n",
       " '1762': 573,\n",
       " '1765': 574,\n",
       " '1767': 575,\n",
       " '177': 576,\n",
       " '1770': 577,\n",
       " '1772': 578,\n",
       " '1776': 579,\n",
       " '1779': 580,\n",
       " '178': 581,\n",
       " '1783': 582,\n",
       " '1784': 583,\n",
       " '1788': 584,\n",
       " '179-run': 585,\n",
       " '1790': 586,\n",
       " '1795': 587,\n",
       " '1796': 588,\n",
       " '17th': 589,\n",
       " '17th-century': 590,\n",
       " '18': 591,\n",
       " '18,00': 592,\n",
       " '18,000': 593,\n",
       " '18,000-strong': 594,\n",
       " '18-a-share': 595,\n",
       " '18-day': 596,\n",
       " '18-million': 597,\n",
       " '18-month': 598,\n",
       " '18-month-old': 599,\n",
       " '18-story': 600,\n",
       " '18-wheeler': 601,\n",
       " '18-year': 602,\n",
       " '18-year-old': 603,\n",
       " '18-years-old': 604,\n",
       " '18.46.09': 605,\n",
       " '18.5': 606,\n",
       " '18.7': 607,\n",
       " '180': 608,\n",
       " '180,000-employee': 609,\n",
       " '1800': 610,\n",
       " '1802': 611,\n",
       " '1804': 612,\n",
       " '1806': 613,\n",
       " '1809': 614,\n",
       " '181': 615,\n",
       " '1810': 616,\n",
       " '1811': 617,\n",
       " '1814': 618,\n",
       " '1815': 619,\n",
       " '1819': 620,\n",
       " '182': 621,\n",
       " '1820s': 622,\n",
       " '1821': 623,\n",
       " '1822': 624,\n",
       " '1824': 625,\n",
       " '1825': 626,\n",
       " '1826': 627,\n",
       " '1827': 628,\n",
       " '1828': 629,\n",
       " '1829': 630,\n",
       " '183': 631,\n",
       " '1830': 632,\n",
       " '1834': 633,\n",
       " '1838': 634,\n",
       " '1839': 635,\n",
       " '184': 636,\n",
       " '1840': 637,\n",
       " '1841': 638,\n",
       " '1843': 639,\n",
       " '1844': 640,\n",
       " '1845': 641,\n",
       " '1848': 642,\n",
       " '185': 643,\n",
       " '185-million': 644,\n",
       " '1850': 645,\n",
       " '1850s': 646,\n",
       " '1851': 647,\n",
       " '1852': 648,\n",
       " '1853': 649,\n",
       " '1854': 650,\n",
       " '1856': 651,\n",
       " '1857': 652,\n",
       " '1859': 653,\n",
       " '186': 654,\n",
       " '1860': 655,\n",
       " '1860s': 656,\n",
       " '1861': 657,\n",
       " '1862': 658,\n",
       " '1863': 659,\n",
       " '1864': 660,\n",
       " '1865': 661,\n",
       " '1866': 662,\n",
       " '1867': 663,\n",
       " '187': 664,\n",
       " '1870': 665,\n",
       " '1870s': 666,\n",
       " '1872': 667,\n",
       " '1874': 668,\n",
       " '1875': 669,\n",
       " '1876': 670,\n",
       " '1878': 671,\n",
       " '188': 672,\n",
       " '1881': 673,\n",
       " '1885': 674,\n",
       " '1886': 675,\n",
       " '1887': 676,\n",
       " '1888': 677,\n",
       " '1889': 678,\n",
       " '1890': 679,\n",
       " '1890s': 680,\n",
       " '1891': 681,\n",
       " '1892': 682,\n",
       " '1895': 683,\n",
       " '1896': 684,\n",
       " '1898': 685,\n",
       " '1899': 686,\n",
       " '18th': 687,\n",
       " '19': 688,\n",
       " '19,000': 689,\n",
       " '19-day': 690,\n",
       " '19-month': 691,\n",
       " '19-nation': 692,\n",
       " '19-year-old': 693,\n",
       " '19.5': 694,\n",
       " '19.7': 695,\n",
       " '19.9': 696,\n",
       " '190': 697,\n",
       " '1900': 698,\n",
       " '1901': 699,\n",
       " '1902': 700,\n",
       " '1903': 701,\n",
       " '1904': 702,\n",
       " '1905': 703,\n",
       " '1906': 704,\n",
       " '1907': 705,\n",
       " '1908': 706,\n",
       " '191': 707,\n",
       " '1910': 708,\n",
       " '1911': 709,\n",
       " '1912': 710,\n",
       " '1914': 711,\n",
       " '1915': 712,\n",
       " '1916': 713,\n",
       " '1917': 714,\n",
       " '1918': 715,\n",
       " '1919': 716,\n",
       " '192': 717,\n",
       " '1920': 718,\n",
       " '1920s': 719,\n",
       " '1921': 720,\n",
       " '1922': 721,\n",
       " '1923': 722,\n",
       " '1924': 723,\n",
       " '1925': 724,\n",
       " '1928': 725,\n",
       " '1929': 726,\n",
       " '193': 727,\n",
       " '1930': 728,\n",
       " '1930s': 729,\n",
       " '1931': 730,\n",
       " '1932': 731,\n",
       " '1933': 732,\n",
       " '1935': 733,\n",
       " '1936': 734,\n",
       " '1937': 735,\n",
       " '1938': 736,\n",
       " '1939': 737,\n",
       " '1940': 738,\n",
       " '1941': 739,\n",
       " '1942': 740,\n",
       " '1943': 741,\n",
       " '1944': 742,\n",
       " '1945': 743,\n",
       " '1946': 744,\n",
       " '1947': 745,\n",
       " '1948': 746,\n",
       " '1949': 747,\n",
       " '195': 748,\n",
       " '1950': 749,\n",
       " '1950s': 750,\n",
       " '1951': 751,\n",
       " '1952': 752,\n",
       " '1953': 753,\n",
       " '1954': 754,\n",
       " '1955': 755,\n",
       " '1956': 756,\n",
       " '1957': 757,\n",
       " '1958': 758,\n",
       " '1959': 759,\n",
       " '196': 760,\n",
       " '1960': 761,\n",
       " '1960s': 762,\n",
       " '1961': 763,\n",
       " '1962': 764,\n",
       " '1963': 765,\n",
       " '1964': 766,\n",
       " '1965': 767,\n",
       " '1966': 768,\n",
       " '1967': 769,\n",
       " '1968': 770,\n",
       " '1969': 771,\n",
       " '197': 772,\n",
       " '197-page': 773,\n",
       " '1970': 774,\n",
       " '1970s': 775,\n",
       " '1971': 776,\n",
       " '1972': 777,\n",
       " '1973': 778,\n",
       " '1974': 779,\n",
       " '1975': 780,\n",
       " '1975-to-1990': 781,\n",
       " '1976': 782,\n",
       " '1977': 783,\n",
       " '1978': 784,\n",
       " '1979': 785,\n",
       " '1980': 786,\n",
       " '1980s': 787,\n",
       " '1981': 788,\n",
       " '1982': 789,\n",
       " '1983': 790,\n",
       " '1984': 791,\n",
       " '1985': 792,\n",
       " '1986': 793,\n",
       " '1987': 794,\n",
       " '1988': 795,\n",
       " '1989': 796,\n",
       " '199': 797,\n",
       " '1990': 798,\n",
       " '1990s': 799,\n",
       " '1991': 800,\n",
       " '1992': 801,\n",
       " '1993': 802,\n",
       " '1994': 803,\n",
       " '1995': 804,\n",
       " '1996': 805,\n",
       " '1997': 806,\n",
       " '1998': 807,\n",
       " '1999': 808,\n",
       " '19th': 809,\n",
       " '1st': 810,\n",
       " '2': 811,\n",
       " '2,00,000': 812,\n",
       " '2,000': 813,\n",
       " '2,000-years-old': 814,\n",
       " '2,012': 815,\n",
       " '2,10,000': 816,\n",
       " '2,100': 817,\n",
       " '2,11,000': 818,\n",
       " '2,17,000': 819,\n",
       " '2,18,000': 820,\n",
       " '2,20,000': 821,\n",
       " '2,200': 822,\n",
       " '2,200-kilometer': 823,\n",
       " '2,25,000': 824,\n",
       " '2,30,000': 825,\n",
       " '2,300': 826,\n",
       " '2,38,000': 827,\n",
       " '2,40,000': 828,\n",
       " '2,400': 829,\n",
       " '2,400-kilometer': 830,\n",
       " '2,46,000': 831,\n",
       " '2,47,000': 832,\n",
       " '2,50,000': 833,\n",
       " '2,500': 834,\n",
       " '2,55,000': 835,\n",
       " '2,60,000': 836,\n",
       " '2,600': 837,\n",
       " '2,633-meter': 838,\n",
       " '2,68,000': 839,\n",
       " '2,698': 840,\n",
       " '2,70,000': 841,\n",
       " '2,700': 842,\n",
       " '2,711': 843,\n",
       " '2,720': 844,\n",
       " '2,75,000': 845,\n",
       " '2,781': 846,\n",
       " '2,80,000': 847,\n",
       " '2,800': 848,\n",
       " '2,900': 849,\n",
       " '2-0': 850,\n",
       " '2-2.5': 851,\n",
       " '2-million': 852,\n",
       " '2.00': 853,\n",
       " '2.07.51': 854,\n",
       " '2.08.50': 855,\n",
       " '2.1': 856,\n",
       " '2.11.37': 857,\n",
       " '2.2': 858,\n",
       " '2.21.40': 859,\n",
       " '2.29': 860,\n",
       " '2.3': 861,\n",
       " '2.3-million-strong': 862,\n",
       " '2.35': 863,\n",
       " '2.38.46': 864,\n",
       " '2.4': 865,\n",
       " '2.42': 866,\n",
       " '2.43': 867,\n",
       " '2.46': 868,\n",
       " '2.5': 869,\n",
       " '2.5-trillion': 870,\n",
       " '2.5-year': 871,\n",
       " '2.6': 872,\n",
       " '2.7': 873,\n",
       " '2.7-million': 874,\n",
       " '2.75': 875,\n",
       " '2.8': 876,\n",
       " '2.9': 877,\n",
       " '2.98': 878,\n",
       " '20': 879,\n",
       " '20,00,000': 880,\n",
       " '20,000': 881,\n",
       " '20-17': 882,\n",
       " '20-25': 883,\n",
       " '20-kilometer': 884,\n",
       " '20-meter': 885,\n",
       " '20-minute': 886,\n",
       " '20-month': 887,\n",
       " '20-percent': 888,\n",
       " '20-thousand': 889,\n",
       " '20-ton': 890,\n",
       " '20-year': 891,\n",
       " '20-year-old': 892,\n",
       " '20-year-olds': 893,\n",
       " '20/20': 894,\n",
       " '200': 895,\n",
       " '200-kilometer': 896,\n",
       " '200-meter': 897,\n",
       " '200-mile': 898,\n",
       " '2000': 899,\n",
       " '2000s': 900,\n",
       " '2001': 901,\n",
       " '2002': 902,\n",
       " '2003': 903,\n",
       " '2004': 904,\n",
       " '2005': 905,\n",
       " '2006': 906,\n",
       " '2007': 907,\n",
       " '2008': 908,\n",
       " '2009': 909,\n",
       " '201': 910,\n",
       " '2010': 911,\n",
       " '2011': 912,\n",
       " '2012': 913,\n",
       " '2013': 914,\n",
       " '2014': 915,\n",
       " '2015': 916,\n",
       " '2016': 917,\n",
       " '2017': 918,\n",
       " '2018': 919,\n",
       " '2019': 920,\n",
       " '202': 921,\n",
       " '202-205-9942': 922,\n",
       " '202.7': 923,\n",
       " '2020': 924,\n",
       " '2022': 925,\n",
       " '2023': 926,\n",
       " '2025': 927,\n",
       " '203': 928,\n",
       " '203-4': 929,\n",
       " '203.1': 930,\n",
       " '2030': 931,\n",
       " '2035': 932,\n",
       " '204': 933,\n",
       " '2047': 934,\n",
       " '2050': 935,\n",
       " '206': 936,\n",
       " '207.5': 937,\n",
       " '20s': 938,\n",
       " '20th': 939,\n",
       " '21': 940,\n",
       " '21,000': 941,\n",
       " '21-Jul': 942,\n",
       " '21-member': 943,\n",
       " '21-month': 944,\n",
       " '21-month-old': 945,\n",
       " '21-year': 946,\n",
       " '21-year-old': 947,\n",
       " '21.3': 948,\n",
       " '210': 949,\n",
       " '2100': 950,\n",
       " '212': 951,\n",
       " '212-to-206': 952,\n",
       " '213': 953,\n",
       " '214': 954,\n",
       " '215': 955,\n",
       " '216': 956,\n",
       " '216-3': 957,\n",
       " '217': 958,\n",
       " '218': 959,\n",
       " '218-5': 960,\n",
       " '21st': 961,\n",
       " '22': 962,\n",
       " '22,000': 963,\n",
       " '22-kilometer': 964,\n",
       " '22-member': 965,\n",
       " '22-month': 966,\n",
       " '22-month-old': 967,\n",
       " '22-nation': 968,\n",
       " '22-year': 969,\n",
       " '22-year-old': 970,\n",
       " '22.4': 971,\n",
       " '22.5': 972,\n",
       " '22.772': 973,\n",
       " '220': 974,\n",
       " '220-kilogram': 975,\n",
       " '221': 976,\n",
       " '223': 977,\n",
       " '224': 978,\n",
       " '225': 979,\n",
       " '225-kilometer-long': 980,\n",
       " '225-member': 981,\n",
       " '225-seat': 982,\n",
       " '227': 983,\n",
       " '227-kilogram': 984,\n",
       " '228': 985,\n",
       " '22nd': 986,\n",
       " '23': 987,\n",
       " '23,000': 988,\n",
       " '23-25': 989,\n",
       " '23-member': 990,\n",
       " '23-year-old': 991,\n",
       " '23.25': 992,\n",
       " '23.30': 993,\n",
       " '23.5': 994,\n",
       " '23.7': 995,\n",
       " '230': 996,\n",
       " '232': 997,\n",
       " '232nd': 998,\n",
       " '233': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "distinct_numbers = np.linspace(0,n_words-1,n_words, dtype=int)\n",
    "word2idx = {}\n",
    "idx2word = {}\n",
    "for i in range(len(distinct_numbers)):\n",
    "    word2idx[words[i]] = distinct_numbers[i]\n",
    "    idx2word[distinct_numbers[i]] = words[i]\n",
    "word2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.6** Prepare the predictors matrix ```X```, as a list of lists, where each inner list is a sequence of words mapped into integers accordly to the ```word2idx``` dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "X = [[] for x in range(len(sentence_list))]\n",
    "for i in range(len(sentence_list)):\n",
    "    X[i] = []\n",
    "    for j in range(len(sentence_list[i])):\n",
    "        (word, tag) = sentence_list[i][j]\n",
    "        X[i].append( word2idx[word])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.7** Apply the keras ```pad_sequences``` function to standardize the predictors. You should retrieve a matrix with all padded sentences and length equal to ```max_len``` previously computed. The dimensionality should therefore be equal to ```[# of sentences, max_len]```. Run the provided cell to print your results. Your ```X[i]``` now should be something similar to this:\n",
    "\n",
    "`[ 8193 27727 31033 33289 22577 33464 23723 16665 33464 31142 31319 28267\n",
    " 27700 33246 28646 16052    21 16915 17349  7924 32879 32985 18238 23555\n",
    "    24 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178\n",
    " 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178\n",
    " 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178\n",
    " 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178\n",
    " 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178\n",
    " 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178\n",
    " 35178 35178 35178 35178 35178 35178 35178 35178]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "X = pad_sequences(X,max_len,value = word2idx['ENDPAD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index of word 'Harvard' is: 7506\n",
      "\n",
      "Sentence 1: [35178 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178\n",
      " 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178\n",
      " 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178\n",
      " 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178\n",
      " 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178\n",
      " 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178\n",
      " 35178 35178  6283 27700 31967 25619 24853 33246 19981 25517 33246 29399\n",
      " 34878 19044 18095 34971 32712 31830 17742     1  4114 11464 11631 14985\n",
      "     1 17364     1 14484 33246  3881    24     1]\n",
      "\n",
      "(47959, 104)\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to show your results #\n",
    "print(\"The index of word 'Harvard' is: {}\\n\".format(word2idx[\"Harvard\"]))\n",
    "print(\"Sentence 1: {}\\n\".format(X[1]))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.8** Create a ```tag2idx``` dictionary mapping distinct named entity tags from the dataset into distinct integers. Also create a ```idx2tag``` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# your code here\n",
    "distinct_numbers = np.linspace(0,n_tags-1,n_tags, dtype=int)\n",
    "tag2idx = {}\n",
    "idx2tag = {}\n",
    "for i in range(len(distinct_numbers)):\n",
    "    tag2idx[tags[i]] = distinct_numbers[i]\n",
    "    idx2tag[distinct_numbers[i]] = tags[i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.9** Prepare targets matrix ```Y```, as a list of lists,where each inner list is a sequence of tags mapped into integers accordly to the ```tag2idx``` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# your code here\n",
    "Y = [[] for x in range(len(sentence_list))]\n",
    "for i in range(len(sentence_list)):\n",
    "    Y[i] = []\n",
    "    for j in range(len(sentence_list[i])):\n",
    "        (word, tag) = sentence_list[i][j]\n",
    "        Y[i].append( tag2idx[tag])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.10** apply the keras ```pad_sequences``` function to standardize the targets. Inject the ```PAD``` tag for the padding words. You should retrieve a matrix with all padded sentences'tags and length equal to ```max_length``` previously computed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your # your code here\n",
    "Y = pad_sequences(Y,max_len, value=tag2idx['PAD'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.11** Use the ```to_categorical``` keras function to one-hot encode the tags. Now your ```Y``` should have dimension ```[# of sentences, max_len, n_tags]```. Run the provided cell to print your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "Y = to_categorical(Y, num_classes = n_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index of tag 'B-gpe' is: 3\n",
      "\n",
      "The tag of the last word in Sentence 1: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "(47959, 104, 18)\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to show your results #\n",
    "print(\"The index of tag 'B-gpe' is: {}\\n\".format(tag2idx[\"B-gpe\"]))\n",
    "print(\"The tag of the last word in Sentence 1: {}\\n\".format(Y[0][-1]))\n",
    "print(np.array(Y).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.12** Split the dataset into train and test sets (test 10%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, Y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Modelling\n",
    "\n",
    "After preparing the train and test sets, we are ready to build five models: \n",
    "* frequency-based baseline \n",
    "* vanilla feedforward neural network\n",
    "* recurrent neural network\n",
    "* gated recurrent neural network\n",
    "* bidirectional gated recurrent neural network\n",
    "\n",
    "More details are given about architecture in each model's section. The input/output dimensionalities will be the same for all models:\n",
    "* input: ```[# of sentences, max_len]```\n",
    "* output: ```[# of sentences, max_len, n_tags]```\n",
    "\n",
    "Follow the information in each model's section to set up the architecture of each model. And the end of each training, use the given ```store_model``` function to store the weights and architectures in the ```./models``` path for later testing;```load_keras_model()``` is also provided to you\n",
    "\n",
    "A further ```plot_training_history``` helper function is given in case you need to check the training history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store model\n",
    "def store_keras_model(model, model_name):\n",
    "    model_json = model.to_json() # serialize model to JSON\n",
    "    with open(\"./models/{}.json\".format(model_name), \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(\"./models/{}.h5\".format(model_name)) # serialize weights to HDF5\n",
    "    print(\"Saved model to disk\")\n",
    "    \n",
    "# Plot history\n",
    "def plot_training_history(history):\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1,len(loss)+1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model \n",
    "def load_keras_model(model_name):\n",
    "    # Load json and create model\n",
    "    json_file = open('./models/{}.json'.format(model_name), 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model = tf.keras.models.model_from_json(loaded_model_json)\n",
    "    # Load weights into new model\n",
    "    model.load_weights(\"./models/{}.h5\".format(model_name))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b>Question 2: Models [40 points total]</b></div>\n",
    "\n",
    "**2.1** **Model 1: Baseline Model**\n",
    "\n",
    "Predict the tag of a word simply with the most frequently-seen named entity tag of this word from the training set.\n",
    "\n",
    "e.g. word \"Apple\" appears 10 times in the training set; 7 times it was tagged as \"Corporate\" and 3 times it was tagged as \"Fruit\". If we encounter the word \"Apple\" in the test set, we predict it as \"Corporate\".\n",
    "\n",
    "**Create an np.array ```baseline``` of length [n_words]**\n",
    "where the ith element   ```baseline[i]``` is the index of the most commonly seen named entity tag of word i summarised from training set.   (e.g. [16, 16, 16, ..., 0, 16, 16])\n",
    "\n",
    "\n",
    "**2.2** **Model 2: Vanilla Feed Forward Neural Network**\n",
    "\n",
    "This model is provided for you. Please pay attention to the architecture of this neural network, especially the input/output dimensionalities and the Embedding layer.\n",
    "\n",
    "\n",
    "**2.2a** Explain what the embedding layer is and why we need it here.\n",
    "\n",
    "**2.2b** Explain why the Param # of Embedding layer is 1758950 (as shown in `print(model.summary())`).\n",
    "\n",
    "**2.3** **Model 3: RNN**\n",
    "\n",
    "Set up a simple RNN model by stacking the following layers in sequence:\n",
    "\n",
    "    an input layer\n",
    "    a simple Embedding layer transforming integer words into vectors\n",
    "    a dropout layer to regularize the model\n",
    "    a SimpleRNN layer\n",
    "    a TimeDistributed layer with an inner Dense layer which output dimensionality is equal to n_tag\n",
    "    \n",
    "*(For hyperparameters, use those provided in Model 2)*\n",
    "\n",
    "**2.3a** Define, compile, and train an RNN model. Use the provided code to save the model and plot the training history.\n",
    "\n",
    "**2.3b** Visualize outputs from the SimpleRNN layer, one subplot for B-tags and one subplot for I-tags. Comment on the patterns you observed.\n",
    "\n",
    "**2.4** **Model 4: GRU**\n",
    "\n",
    "**2.4a** Briefly explain what a GRU is and how it's different from a simple RNN.\n",
    "\n",
    "**2.4b** Define, compile, and train a GRU architecture by replacing the SimpleRNN cell with a GRU one. Use the provided code to save the model and plot the training history.\n",
    "\n",
    "**2.4c** Visualize outputs from GRU layer, one subplot for **B-tags** and one subplot for **I-tags**. Comment on the patterns you observed.\n",
    "\n",
    "**2.5** **Model 5: Bidirectional GRU**\n",
    "\n",
    "**2.5a** Explain how a Bidirectional GRU differs from GRU model above.\n",
    "\n",
    "**2.5b** Define, compile, and train a bidirectional GRU by wrapping your GRU layer in a Bidirectional one. Use the provided code to save the model and plot the training history.\n",
    "\n",
    "**2.5c** Visualize outputs from bidirectional GRU layer, one subplot for **B-tags** and one subplot for **I-tags**. Comment on the patterns you observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hBtmANNuuS6h"
   },
   "source": [
    "## Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1** **Model 1: Baseline Model**\n",
    "\n",
    "Predict the tag of a word simply with the most frequently-seen named entity tag of this word from the training set.\n",
    "\n",
    "e.g. word \"Apple\" appears 10 times in the training set; 7 times it was tagged as \"Corporate\" and 3 times it was tagged as \"Fruit\". If we encounter the word \"Apple\" in the test set, we predict it as \"Corporate\".\n",
    "\n",
    "**Create an np.array ```baseline``` of length [n_words]**\n",
    "where the ith element   ```baseline[i]``` is the index of the most commonly seen named entity tag of word i summarised from training set.   (e.g. [16, 16, 16, ..., 0, 16, 16])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "baseline = np.zeros((n_words,n_tags))\n",
    "percent = 0\n",
    "for i in range(X_tr.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "        word_id = X_tr[i,j]\n",
    "        baseline[word_id,:] += y_tr[i,j,:]\n",
    "\n",
    "\n",
    "new_baseline = np.zeros((n_words,1))\n",
    "for i in range(n_words):\n",
    "    if np.sum(baseline[i,:]) == 0:\n",
    "        tag = tag2idx['O']\n",
    "    else: \n",
    "        tag = np.argmax(baseline[i,:])\n",
    "    new_baseline[i] = tag\n",
    "baseline = np.squeeze(new_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47959, 104) \n",
      "\n",
      "Sentence:\n",
      " ['ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'ENDPAD', 'Thousands', 'of', 'demonstrators', 'have', 'marched', 'through', 'London', 'to', 'protest', 'the', 'war', 'in', 'Iraq', 'and', 'demand', 'the', 'withdrawal', 'of', 'British', 'troops', 'from', 'that', 'country', '.']\n",
      "\n",
      "Predicted Tags:\n",
      " ['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to show your results #\n",
    "print(baseline[X].shape,'\\n')\n",
    "print('Sentence:\\n {}\\n'.format([idx2word[w] for w in X[0]]))\n",
    "print('Predicted Tags:\\n {}'.format([idx2tag[i] for i in baseline[X[0]]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2** **Model 2: Vanilla Feed Forward Neural Network**\n",
    "\n",
    "This model is provided for you. Please pay attention to the architecture of this neural network, especially the input/output dimensionalities and the Embedding layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use these hyperparameters for all NN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_units = 100\n",
    "drop_rate = .1\n",
    "dim_embed = 50\n",
    "\n",
    "optimizer = \"rmsprop\"\n",
    "loss = \"categorical_crossentropy\"\n",
    "metrics = [\"accuracy\"]\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "validation_split = 0.1\n",
    "verbose = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(input_dim=n_words, output_dim=dim_embed, input_length=max_len))\n",
    "model.add(tf.keras.layers.Dropout(drop_rate))\n",
    "model.add(tf.keras.layers.Dense(n_tags, activation=\"softmax\"))\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 104, 50)           1758950   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 104, 50)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 104, 18)           918       \n",
      "=================================================================\n",
      "Total params: 1,759,868\n",
      "Trainable params: 1,759,868\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 38846 samples, validate on 4317 samples\n",
      "Epoch 1/10\n",
      "38846/38846 [==============================] - 16s 414us/sample - loss: 0.3146 - accuracy: 0.9786 - val_loss: 0.0599 - val_accuracy: 0.9858\n",
      "Epoch 2/10\n",
      "38846/38846 [==============================] - 14s 353us/sample - loss: 0.0515 - accuracy: 0.9863 - val_loss: 0.0482 - val_accuracy: 0.9868\n",
      "Epoch 3/10\n",
      "38846/38846 [==============================] - 14s 356us/sample - loss: 0.0451 - accuracy: 0.9872 - val_loss: 0.0456 - val_accuracy: 0.9873\n",
      "Epoch 4/10\n",
      "38846/38846 [==============================] - 14s 354us/sample - loss: 0.0428 - accuracy: 0.9877 - val_loss: 0.0442 - val_accuracy: 0.9877\n",
      "Epoch 5/10\n",
      "38846/38846 [==============================] - 14s 353us/sample - loss: 0.0415 - accuracy: 0.9881 - val_loss: 0.0434 - val_accuracy: 0.9879\n",
      "Epoch 6/10\n",
      "38846/38846 [==============================] - 14s 355us/sample - loss: 0.0406 - accuracy: 0.9883 - val_loss: 0.0430 - val_accuracy: 0.9881\n",
      "Epoch 7/10\n",
      "38846/38846 [==============================] - 14s 355us/sample - loss: 0.0400 - accuracy: 0.9885 - val_loss: 0.0427 - val_accuracy: 0.9882\n",
      "Epoch 8/10\n",
      "38846/38846 [==============================] - 14s 356us/sample - loss: 0.0396 - accuracy: 0.9886 - val_loss: 0.0425 - val_accuracy: 0.9882\n",
      "Epoch 9/10\n",
      "38846/38846 [==============================] - 14s 356us/sample - loss: 0.0393 - accuracy: 0.9887 - val_loss: 0.0423 - val_accuracy: 0.9883\n",
      "Epoch 10/10\n",
      "38846/38846 [==============================] - 14s 359us/sample - loss: 0.0390 - accuracy: 0.9888 - val_loss: 0.0422 - val_accuracy: 0.9883\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "history = model.fit(X_tr, y_tr, batch_size=batch_size, epochs=epochs, \n",
    "                    validation_split=validation_split, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "store_keras_model(model, 'model_FFNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2a** Explain what the embedding layer is and why we need it here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2b** Explain why the Param # of Embedding layer is 1758950 (as shown in `print(model.summary())`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing Hidden Layers\n",
    "In addition to the final result, we also want to see the intermediate results from hidden layers. Below is an example showing how to get outputs from a hidden layer, and visualize them on the reduced dimension of 2D by PCA. (**Please note that this code and the parameters cannot be simply copied and pasted for other questions; some adjustments need to be made**) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFNN = load_keras_model(\"model_FFNN\")\n",
    "def create_truncated_model_FFNN(trained_model):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(input_dim=n_words, output_dim=dim_embed, input_length=max_len))\n",
    "    model.add(tf.keras.layers.Dropout(drop_rate))\n",
    "    # set weights of first few layers using the weights of trained model\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        layer.set_weights(trained_model.layers[i].get_weights())\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    return model\n",
    "truncated_model = create_truncated_model_FFNN(FFNN)\n",
    "hidden_features = truncated_model.predict(X_te)\n",
    "\n",
    "# flatten data\n",
    "hidden_features = hidden_features.reshape(-1,50)\n",
    " \n",
    "# find first two PCA components\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(hidden_features)\n",
    "print('Variance explained by PCA: {}'.format(np.sum(pca.explained_variance_ratio_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize hidden featurs on first two PCA components\n",
    "# this plot only shows B-tags\n",
    "def visualize_hidden_features(pca_result):\n",
    "    color=['r', 'C1', 'y', 'C3', 'b', 'g', 'm', 'orange']\n",
    "    category = np.argmax(y_te, axis=1)\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(6,6)    \n",
    "    for cat in range(8):\n",
    "        indices_B = np.where(category==cat)[0]\n",
    "        #length=min(1000,len(indices_B))\n",
    "        #indices_B=indices_B[:length]\n",
    "        ax.scatter(pca_result[indices_B,0], pca_result[indices_B, 1], label=idx2tag[cat],s=2,color=color[cat],alpha=0.5)\n",
    "        legend=ax.legend(markerscale=3)\n",
    "        legend.get_frame().set_facecolor('w')  \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_hidden_features(pca_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full function for other questions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hidden_output_PCA(model,X_te,y_te,layer_index,out_dimension):\n",
    "    output = tf.keras.backend.function([model.layers[0].input],[model.layers[layer_index].output])\n",
    "    hidden_feature=np.array(output([X_te]))\n",
    "    hidden_feature=hidden_feature.reshape(-1,out_dimension)\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result = pca.fit_transform(hidden_feature)\n",
    "    print('Variance explained by PCA: {}'.format(np.sum(pca.explained_variance_ratio_)))\n",
    "    return pca_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_B_I(pca_result):\n",
    "    color = ['r', 'C1', 'y', 'C3', 'b', 'g', 'm', 'orange']\n",
    "    category = np.argmax(y_te.reshape(-1,18), axis=1)\n",
    "    fig, ax = plt.subplots(1,2) \n",
    "    fig.set_size_inches(12,6)\n",
    "    for i in range(2):\n",
    "        for cat in range(8*i,8*(i+1)):\n",
    "            indices = np.where(category==cat)[0]\n",
    "            ax[i].scatter(pca_result[indices,0], pca_result[indices, 1], label=idx2tag[cat],s=2,color=color[cat-8*i],alpha=0.5)\n",
    "        legend = ax[i].legend(markerscale=3)\n",
    "        legend.get_frame().set_facecolor('w') \n",
    "        ax[i].set_xlabel(\"first dimension\")\n",
    "        ax[i].set_ylabel(\"second dimension\")\n",
    "    fig.suptitle(\"visualization of hidden feature on reduced dimension by PCA\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = get_hidden_output_PCA(FFNN,X_te,y_te,1,50)\n",
    "visualize_B_I(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3** **Model 3: RNN**\n",
    "\n",
    "Set up a simple RNN model by stacking the following layers in sequence:\n",
    "\n",
    "    an input layer\n",
    "    a simple Embedding layer transforming integer words into vectors\n",
    "    a dropout layer to regularize the model\n",
    "    a SimpleRNN layer\n",
    "    a TimeDistributed layer with an inner Dense layer which output dimensionality is equal to n_tag\n",
    "    \n",
    "*(For hyperparameters, use those provided in Model 2)*\n",
    "\n",
    "**2.3a** Define, compile, and train an RNN model. Use the provided code to save the model and plot the training history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your mode #\n",
    "store_keras_model(model, 'model_RNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to show your results #\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to show your results #\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3b** Visualize outputs from the SimpleRNN layer, one subplot for B-tags and one subplot for I-tags. Comment on the patterns you observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='explication'> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4** **Model 4: GRU**\n",
    "\n",
    "**2.4a** Briefly explain what a GRU is and how it's different from a simple RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4b** Define, compile, and train a GRU architecture by replacing the SimpleRNN cell with a GRU one. Use the provided code to save the model and plot the training history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your model #\n",
    "store_keras_model(model, 'model_GRU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to show your results #\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to show your results #\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4c** Visualize outputs from GRU layer, one subplot for **B-tags** and one subplot for **I-tags**. Comment on the patterns you observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.5** **Model 5: Bidirectional GRU**\n",
    "\n",
    "**2.5a** Explain how a Bidirectional GRU differs from GRU model above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.5b** Define, compile, and train a bidirectional GRU by wrapping your GRU layer in a Bidirectional one. Use the provided code to save the model and plot the training history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your model #\n",
    "store_keras_model(model, 'model_BiGRU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to show your results #\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to show your results #\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.5c** Visualize outputs from bidirectional GRU layer, one subplot for **B-tags** and one subplot for **I-tags**. Comment on the patterns you observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Question 3: Analysis [40pt]</b></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1** For each model, iteratively:\n",
    "\n",
    "- Load the model using the given function ```load_keras_model```\n",
    "\n",
    "- Apply the model to the test dataset\n",
    "\n",
    "- Compute an F1 score for each ```Tag``` and store it \n",
    "\n",
    "**3.2** Plot the F1 score per Tag and per model making use of a grouped barplot.\n",
    "\n",
    "**3.3** Briefly discuss the performance of each model\n",
    "\n",
    "\n",
    "**3.4** Which tags have the lowest f1 score? For instance, you may find from the plot above that the test accuracy on \"B-art\", and \"I-art\" are very low (just an example, your case maybe different). Here is an example when models failed to predict these tags right\n",
    "\n",
    "<img src=\"data/B_art.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "**3.5** Write functions to output another example in which the tags of the lowest accuracy was predicted wrong in a sentence (include both \"B-xxx\" and \"I-xxx\" tags). Store the results in a DataFrame (same format as the above example) and use styling functions below to print out your df.\n",
    "\n",
    "**3.6** Choose one of the most promising models you have built, improve this model to achieve an f1 score higher than 0.8 for as many tags as possible (you have lots of options here, e.g. data balancing, hyperparameter tuning, changing the structure of NN, a different optimizer, etc.)\n",
    "\n",
    "**3.7** Explain why you chose to change certain elements of the model and how effective these adjustments were.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hBtmANNuuS6h"
   },
   "source": [
    "## Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1** For each model, iteratively:\n",
    "\n",
    "- Load the model using the given function ```load_keras_model```\n",
    "\n",
    "- Apply the model to the test dataset\n",
    "\n",
    "- Compute an F1 score for each ```Tag``` and store it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2** Plot the F1 score per Tag and per model making use of a grouped barplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3** Briefly discuss the performance of each model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4** Which tags have the lowest f1 score? For instance, you may find from the plot above that the test accuracy on \"B-art\", and \"I-art\" are very low (just an example, your case maybe different). Here is an example when models failed to predict these tags right\n",
    "\n",
    "<img src=\"data/B_art.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.5** Write functions to output another example in which the tags of the lowest accuracy was predicted wrong in a sentence (include both \"B-xxx\" and \"I-xxx\" tags). Store the results in a DataFrame (same format as the above example) and use styling functions below to print out your df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_errors(s):\n",
    "    is_max = s == s.y_true\n",
    "    return ['' if v or key=='Word' else 'color: red' for key,v in is_max.iteritems()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.6** Choose one of the most promising models you have built, improve this model to achieve an f1 score higher than 0.8 for as many tags as possible (you have lots of options here, e.g. data balancing, hyperparameter tuning, changing the structure of NN, a different optimizer, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.7** Explain why you chose to change certain elements of the model and how effective these adjustments were."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
